<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HA | klwang's Notes]]></title>
  <link href="http://blog.klwang.info/blog/categories/ha/atom.xml" rel="self"/>
  <link href="http://blog.klwang.info/"/>
  <updated>2013-06-23T00:31:21+08:00</updated>
  <id>http://blog.klwang.info/</id>
  <author>
    <name><![CDATA[wklxd]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[将 resouce angent 改造成 multi state 类型]]></title>
    <link href="http://blog.klwang.info/blog/change-your-resouce-angent-to-multistate/"/>
    <updated>2013-06-10T00:00:00+08:00</updated>
    <id>http://blog.klwang.info/blog/change-your-resouce-angent-to-multistate</id>
    <content type="html"><![CDATA[<p>习惯了资源clone的方便快捷</p>




<p>突然发现，有时候我们需要在这些无差异的clone资源中选出一个来，好做一些“不一样“的事情，</p>




<p>就好像是在一群人中选出一个代表，来做一些领导性的工作一样</p>




<p>怎么样来做到这点呢？</p>




<p>pacemaker提供了多态(multi-state)的概念，除了具备clone的特性外，还多出了</p>




<p>master/slave的概念，可以在资源agent中判断自己所处的角色，然后根据角色的不同，做一些不同的操作</p>




<p>那么，如何将自己的agent改造为 master/slave 的呢？</p>




<p>最简单的思路：在每个节点写一个文件，记录下自己所处的角色好了</p>




<p>有了思路就可以开始实施了，这里完全照搬 pacemaker/Stateful 资源的做法</p>




<p>改造开始</p>




<p>1. 设置 $CRM_MASTER</p>




<p><pre>
    在 # Initialization: 块中加入
    CRM_MASTER="${HA_SBIN_DIR}/crm_master -l reboot"
</pre></p>




<p>2. usage 函数改造[可选]</p>




<p>usage函数本来就是给“人”看的，就算不修改，也不会影响功能。可以加入以下的内容对demote和promote进行说明
<pre>
    The 'promote' operation xxxxxx xxxxxx.
    The 'demote' operation xxx xxxxxx.
</pre></p>




<p>3. meta_data函数处理</p>




<p>加入状态文件的参数
<pre>
<parameter name="state" unique="1">
<longdesc lang="en">
Location to store the resource state in
</longdesc>
<shortdesc lang="en">State file</shortdesc>
<content type="string" default="${HA_VARRUN}/Stateful-{OCF_RESOURCE_INSTANCE}.state" />
</parameter>
</pre></p>




<p>action模块加入monitor的动作</p>




<p><pre>
<action name="monitor" depth="0" timeout="20" interval="30" role="Master" />
<action name="monitor" depth="0" timeout="20" interval="30" role="Slave" />
</pre>
注意，默认timeout相同时， pacemaker会认为两个timeout是一个操作，需要在crm中进行修改</p>




<p>下面是pacemaker对设置不同interval的说明
<pre>
It is crucial that every monitor operation has a different interval!
This is because Pacemaker currently differentiates between operations only 
by resource and interval;
so if eg. a master/slave resource has the same monitor interval for both roles, 
Pacemaker would ignore the role when checking the status - which would 
cause unexpected return codes,and therefore unnecessary complications.
</pre></p>




<p>4. 添加 demote 和 promote 函数</p>




<p><pre>
    stateful_update() {
        echo $1 > ${OCF_RESKEY_state}
    }</pre></p>




<p>    stateful_check_state() {<br />
        target=$1<br />
        if [ -f ${OCF_RESKEY_state} ]; then<br />
            state=`cat ${OCF_RESKEY_state}`<br />
            if [ "x$target" = "x$state" ]; then<br />
                return 0<br />
            fi</p>




<p>        else<br />
            if [ "x$target" = "x" ]; then<br />
                return 0<br />
            fi<br />
        fi</p>




<p>        return 1<br />
    }</p>




<p>    stateful_demote() {<br />
        stateful_check_state<br />
        if [ $? = 0 ]; then<br />
            # CRM Error - Should never happen<br />
            return $OCF_NOT_RUNNING<br />
        fi<br />
        stateful_update "slave"<br />
        $CRM_MASTER -v ${slave_score}<br />
        return $OCF_SUCCESS<br />
    }</p>




<p>    stateful_promote() {<br />
        stateful_check_state<br />
        if [ $? = 0 ]; then<br />
            return $OCF_NOT_RUNNING<br />
        fi<br />
        stateful_update "master"<br />
        $CRM_MASTER -v ${master_score}<br />
        return $OCF_SUCCESS<br />
    }

对于 master/slave 的agent，必须要有 promote 和 demote 函数</p>




<p>5. start 函数改造</p>




<p><pre>
    返回之前添加
    stateful_update "slave"
    $CRM_MASTER -v ${slave_score}
</pre>
标记自己的状态为slave，顺便告诉pacemaker(使用crm_master来实现)</p>




<p><strong>注意：master/slave资源启动的角色必须是 slave， 然后由pacemaker在slave的节点中选择一个promote为master</strong></p>




<p>6 stop 函数改造</p>




<p><pre>
    $CRM_MASTER -D
    stateful_check_state "master"
    if [ $? = 0 ]; then
        # CRM Error - Should never happen
        return $OCF_RUNNING_MASTER
    fi
    if [ -f ${OCF_RESKEY_state} ]; then
        rm ${OCF_RESKEY_state}
    fi
</pre>
在发送停止命令之前，先进行demote操作</p>




<p>7 monitor函数改造</p>




<p>    将返回的 $OCF_SUCCESS 那一行修改为
<pre>
    stateful_check_state "master"
    if [ $? = 0 ]; then
        if [ $OCF_RESKEY_CRM_meta_interval = 0 ]; then
            # Restore the master setting during probes
            $CRM_MASTER -v ${master_score}
        fi
        return $OCF_RUNNING_MASTER
    fi</pre></p>




<p>    stateful_check_state "slave"<br />
    if [ $? = 0 ]; then<br />
        if [ $OCF_RESKEY_CRM_meta_interval = 0 ]; then<br />
            # Restore the master setting during probes<br />
            $CRM_MASTER -v ${slave_score}<br />
        fi<br />
        return $OCF_SUCCESS<br />
    fi</p>




<p>    echo "File '${OCF_RESKEY_state}' exists but contains unexpected contents"<br />
    return $OCF_ERR_GENERIC

pacemaker充分相信agent，它对资源角色的判断完全来自monitor函数，所有只要我们告诉它自己是 $OCF_RUNNING_MASTER， 它就会认为我们是master</p>




<p>8 加入默认值</p>




<p><pre>
: ${slave_score=5}
: ${master_score=10}</pre></p>




<p>: ${OCF_RESKEY_CRM_meta_interval=0}<br />
: ${OCF_RESKEY_CRM_meta_globally_unique:="true"}</p>




<p>if [ "x$OCF_RESKEY_state" = "x" ]; then<br />
    if [ ${OCF_RESKEY_CRM_meta_globally_unique} = "false" ]; then<br />
        state="${HA_VARRUN}/Stateful-${OCF_RESOURCE_INSTANCE}.state"</p>




<p>        # Strip off the trailing clone marker<br />
        OCF_RESKEY_state=`echo $state | sed s/:[0-9][0-9]*\.state/.state/`<br />
    else<br />
        OCF_RESKEY_state="${HA_VARRUN}/Stateful-${OCF_RESOURCE_INSTANCE}.state"<br />
    fi<br />
fi

上面的操作中多次用到了 slave_score 和 master_score 等，现在就对它们进行赋值，要点：只要 master > slave 即可</p>




<p>9 最后，加入动作</p>




<p><pre>
case $1 in</pre></p>




<p>    promote)        stateful_promote<br />
            ;;<br />
    demote)         stateful_demote<br />
            ;;
</p>




<p>好了，agent已经改造完成了，现在展示一下</p>




<p>在pacemaker中的配置如下
<pre>
primitive testklwang ocf:test:klwang \
    op monitor interval="30" role="Master" \
    op monitor interval="29" role="Slave"
</pre></p>




<p>下面是crm_mon的结果：
<pre>
Last updated: Sat Jun  8 12:28:15 2013
Last change: Sat Jun  8 12:28:14 2013 via cibadmin on node1
Stack: cman
Current DC: cent1 - partition with quorum
Version: 1.1.8-7.el6-394e906
5 Nodes configured, 3 expected votes
32 Resources configured.</pre></p>




<p>Online: [ node1 node2 node3 node4 node5 ]</p>




<p> Master/Slave Set: ms_klwang [testklwang]<br />
     Masters: [ node1 ]<br />
     Slaves: [ node2 node3 node4 node5 ]
</p>




<p>现在，我们的目的达到了，已经在集群中选出了一个作为master的节点，下来我们就可以做一些不一样的事情啦</p>




<p><pre>
    stateful_check_state "master"
    if [ $? -eq 0  ]; then
        # 将你想干的事情放在这里
        return $OCF_SUCCESS
    fi
</pre></p>




<p>上面演示中 test:klwang 是使用自带的Dummy示例改造而来，把代码贴出来，也好做个对照
<pre>
#!/bin/sh
#
#
#   Dummy OCF RA. Does nothing but wait a few seconds, can be
#   configured to fail occassionally.
#
# Copyright (c) 2004 SUSE LINUX AG, Lars Marowsky-Brée
#                    All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of version 2 of the GNU General Public License as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it would be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
#
# Further, this software is distributed without any warranty that it is
# free of the rightful claim of any third person regarding infringement
# or the like.  Any license provided herein, whether implied or
# otherwise, applies only to this software file.  Patent licenses, if
# any, provided herein do not apply to combinations of this program with
# other software, or any other product whatsoever.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write the Free Software Foundation,
# Inc., 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
#</pre></p>




<p>#######################################################################<br />
# Initialization:</p>




<p>: ${OCF_FUNCTIONS=${OCF_ROOT}/resource.d/heartbeat/.ocf-shellfuncs}<br />
. ${OCF_FUNCTIONS}<br />
: ${__OCF_ACTION=$1}<br />
CRM_MASTER="${HA_SBIN_DIR}/crm_master -l reboot"</p>




<p>#######################################################################</p>




<p><p>meta_data() {<br /></p>

<pre><code>cat &lt;&lt;END
</code></pre>

<p>&lt;?xml version=&ldquo;1.0&rdquo;?>
&lt;!DOCTYPE resource-agent SYSTEM &ldquo;ra-api-1.dtd&rdquo;>
&lt;resource-agent name=&ldquo;Dummy&rdquo; version=&ldquo;1.0&rdquo;>
<version>1.0</version>&lt;/resource-agent></p></p>

<p><p><longdesc lang="en">
This is a Dummy Resource Agent. It does absolutely nothing except <br />
keep track of whether its running or not.<br />
Its purpose in life is for testing and to serve as a template for RA writers.</longdesc></p></p>

<p><p>NB: Please pay attention to the timeouts specified in the actions<br />
section below. They should be meaningful for the kind of resource<br />
the agent manages. They should be the minimum advised timeouts,<br />
but they shouldn&rsquo;t/cannot cover <em>all</em> possible resource<br />
instances. So, try to be neither overly generous nor too stingy,<br />
but moderate. The minimum timeouts should never be below 10 seconds.</p>

<p><shortdesc lang="en">Example stateless resource agent</shortdesc></p></p>

<p><p><parameters>
<parameter name="state" unique="1">
<longdesc lang="en">
Location to store the resource state in.
</longdesc>
<shortdesc lang="en">State file</shortdesc>
<content type="string" default="${HA_VARRUN}/Dummy-{OCF_RESOURCE_INSTANCE}.state" />
</parameter></parameters></p></p>

<p><p><parameter name="fake" unique="0">
<longdesc lang="en">
Fake attribute that can be changed to cause a reload
</longdesc>
<shortdesc lang="en">Fake attribute that can be changed to cause a reload</shortdesc>
<content type="string" default="dummy" />
</parameter></p></p>

<p><p><parameter name="op_sleep" unique="1">
<longdesc lang="en">
Number of seconds to sleep during operations.  This can be used to test how<br />
the cluster reacts to operation timeouts.
</longdesc>
<shortdesc lang="en">Operation sleep duration in seconds.</shortdesc>
<content type="string" default="0" />
</parameter></p></p>

<p><p><parameter name="stateful" unique="1">
<longdesc lang="en">
Location to store the resource stateful in
</longdesc>
<shortdesc lang="en">stateful file</shortdesc>
<content type="string" default="${HA_VARRUN}/Dummy-{OCF_RESOURCE_INSTANCE}.stateful" />
</parameter></p></p>

<p><p></p></p>

<p><p><actions>
<action name="start" timeout="20" />
<action name="stop" timeout="20" />
<action name="monitor" depth="0" timeout="20" interval="30" role="Master" />
<action name="monitor" depth="0" timeout="20" interval="30" role="Slave" />
<action name="reload" timeout="20" />
<action name="migrate_to" timeout="20" />
<action name="migrate_from" timeout="20" />
<action name="validate-all" timeout="20" />
<action name="meta-data" timeout="5" />
</actions></p>

<p>END<br />
}</p></p>

<p><p>#######################################################################</p></p>

<p><p># don&rsquo;t exit on TERM, to test that lrmd makes sure that we do exit<br />
trap sigterm_handler TERM<br />
sigterm_handler() {<br /></p>

<pre><code>ocf_log info "They use TERM to bring us down. No such luck."&lt;br /&gt;
return&lt;br /&gt;
</code></pre>

<p>}</p></p>

<p><p>dummy_usage() {<br /></p>

<pre><code>cat &lt;&lt;END&lt;br /&gt;
</code></pre>

<p>usage: $0 {start|stop|monitor|migrate_to|migrate_from|validate-all|meta-data}</p></p>

<p><p>Expects to have a fully populated OCF RA-compliant environment set.<br />
END<br />
}</p></p>

<p><p>stateful_update() {<br /></p>

<pre><code>echo $1 &gt; ${OCF_RESKEY_state}&lt;br /&gt;
</code></pre>

<p>}</p></p>

<p><p>stateful_check_state() {<br /></p>

<pre><code>target=$1&lt;br /&gt;
if [ -f ${OCF_RESKEY_state} ]; then&lt;br /&gt;
    state=`cat ${OCF_RESKEY_state}`&lt;br /&gt;
    if [ "x$target" = "x$state" ]; then&lt;br /&gt;
        return 0&lt;br /&gt;
    fi&lt;/p&gt;
</code></pre>

<p><p> else<br /></p>

<pre><code>    if [ "x$target" = "x" ]; then&lt;br /&gt;
        return 0&lt;br /&gt;
    fi&lt;br /&gt;
fi&lt;/p&gt;
</code></pre>

<p><p> return 1<br />
}</p></p>

<p><p>stateful_demote() {<br /></p>

<pre><code>stateful_check_state&lt;br /&gt;
if [ $? = 0 ]; then&lt;br /&gt;
    # CRM Error - Should never happen&lt;br /&gt;
    return $OCF_NOT_RUNNING&lt;br /&gt;
fi&lt;br /&gt;
stateful_update "slave"&lt;br /&gt;
$CRM_MASTER -v ${slave_score}&lt;br /&gt;
return $OCF_SUCCESS&lt;br /&gt;
</code></pre>

<p>}</p></p>

<p><p>stateful_promote() {<br /></p>

<pre><code>stateful_check_state&lt;br /&gt;
if [ $? = 0 ]; then&lt;br /&gt;
    return $OCF_NOT_RUNNING&lt;br /&gt;
fi&lt;br /&gt;
stateful_update "master"&lt;br /&gt;
$CRM_MASTER -v ${master_score}&lt;br /&gt;
return $OCF_SUCCESS&lt;br /&gt;
</code></pre>

<p>}</p></p>

<p><p>dummy_start() {<br /></p>

<pre><code>dummy_monitor&lt;br /&gt;
if [ $? =  $OCF_SUCCESS ]; then&lt;br /&gt;
    return $OCF_SUCCESS&lt;br /&gt;
fi&lt;br /&gt;
touch ${OCF_RESKEY_state}&lt;br /&gt;
stateful_update "slave"&lt;br /&gt;
$CRM_MASTER -v ${slave_score}&lt;br /&gt;
</code></pre>

<p>}</p></p>

<p><p>dummy_stop() {<br /></p>

<pre><code>dummy_monitor&lt;br /&gt;
if [ $? = $OCF_SUCCESS ]; then&lt;br /&gt;
    $CRM_MASTER -D&lt;br /&gt;
    stateful_check_state "master"&lt;br /&gt;
    if [ $? = 0 ]; then&lt;br /&gt;
        # CRM Error - Should never happen&lt;br /&gt;
        return $OCF_RUNNING_MASTER&lt;br /&gt;
    fi&lt;br /&gt;
    if [ -f ${OCF_RESKEY_state} ]; then&lt;br /&gt;
        rm ${OCF_RESKEY_state}&lt;br /&gt;
    fi&lt;br /&gt;
    rm ${OCF_RESKEY_state}&lt;br /&gt;
fi&lt;br /&gt;
return $OCF_SUCCESS&lt;br /&gt;
</code></pre>

<p>}</p></p>

<p><p>dummy_monitor() {<br /></p>

<pre><code># Monitor _MUST!_ differentiate correctly between running&lt;br /&gt;
# (SUCCESS), failed (ERROR) or _cleanly_ stopped (NOT RUNNING).&lt;br /&gt;
# That is THREE states, not just yes/no.&lt;/p&gt;
</code></pre>

<p><p> sleep ${OCF_RESKEY_op_sleep}<br /></p>

<pre><code>if [ -f ${OCF_RESKEY_state} ]; then&lt;br /&gt;
        stateful_check_state "master"&lt;br /&gt;
        if [ $? = 0 ]; then&lt;br /&gt;
            if [ $OCF_RESKEY_CRM_meta_interval = 0 ]; then&lt;br /&gt;
                # Restore the master setting during probes&lt;br /&gt;
                $CRM_MASTER -v ${master_score}&lt;br /&gt;
            fi&lt;br /&gt;
            return $OCF_RUNNING_MASTER&lt;br /&gt;
        fi&lt;/p&gt;
</code></pre>

<p><p>         stateful_check_state &ldquo;slave&rdquo;<br /></p>

<pre><code>        if [ $? = 0 ]; then&lt;br /&gt;
            if [ $OCF_RESKEY_CRM_meta_interval = 0 ]; then&lt;br /&gt;
                # Restore the master setting during probes&lt;br /&gt;
                $CRM_MASTER -v ${slave_score}&lt;br /&gt;
            fi&lt;br /&gt;
            return $OCF_SUCCESS&lt;br /&gt;
        fi&lt;br /&gt;
    &lt;br /&gt;
        echo "File '${OCF_RESKEY_state}' exists but contains unexpected contents"&lt;br /&gt;
        return $OCF_ERR_GENERIC&lt;br /&gt;
fi&lt;br /&gt;
if false ; then&lt;br /&gt;
    return $OCF_ERR_GENERIC&lt;br /&gt;
fi&lt;br /&gt;
return $OCF_NOT_RUNNING&lt;br /&gt;
</code></pre>

<p>}</p></p>

<p><p>dummy_validate() {<br /></p>

<pre><code>&lt;br /&gt;
# Is the state directory writable? &lt;br /&gt;
state_dir=`dirname "$OCF_RESKEY_state"`&lt;br /&gt;
touch "$state_dir/$$"&lt;br /&gt;
if [ $? != 0 ]; then&lt;br /&gt;
return $OCF_ERR_ARGS&lt;br /&gt;
fi&lt;br /&gt;
rm "$state_dir/$$"&lt;/p&gt;
</code></pre>

<p><p>    return $OCF_SUCCESS<br />
}</p></p>

<p><p>: ${slave_score=5}<br />
: ${master_score=10}<br />
: ${OCF_RESKEY_fake=dummy}<br />
: ${OCF_RESKEY_op_sleep=0}<br />
: ${OCF_RESKEY_CRM_meta_interval=0}<br />
: ${OCF_RESKEY_CRM_meta_globally_unique:=&ldquo;true&rdquo;}</p></p>

<p><p>if [ &ldquo;x$OCF_RESKEY_state&rdquo; = &ldquo;x&rdquo; ]; then<br /></p>

<pre><code>if [ ${OCF_RESKEY_CRM_meta_globally_unique} = "false" ]; then&lt;br /&gt;
state="${HA_VARRUN}/Dummy-${OCF_RESOURCE_INSTANCE}.state"&lt;br /&gt;

# Strip off the trailing clone marker&lt;br /&gt;
OCF_RESKEY_state=`echo $state | sed s/:[0-9][0-9]*\.state/.state/`&lt;br /&gt;
else &lt;br /&gt;
OCF_RESKEY_state="${HA_VARRUN}/Dummy-${OCF_RESOURCE_INSTANCE}.state"&lt;br /&gt;
fi&lt;br /&gt;
</code></pre>

<p>fi</p></p>

<p><p>case $__OCF_ACTION in<br />
meta-data)  meta_data<br /></p>

<pre><code>    exit $OCF_SUCCESS&lt;br /&gt;
    ;;&lt;br /&gt;
</code></pre>

<p>start)      dummy_start;;<br />
stop)       dummy_stop;;<br />
monitor)    dummy_monitor;;<br />
migrate_to) ocf_log info &ldquo;Migrating ${OCF_RESOURCE_INSTANCE} to ${OCF_RESKEY_CRM_meta_migrate_target}.&rdquo;<br /></p>

<pre><code>        dummy_stop&lt;br /&gt;
    ;;&lt;br /&gt;
</code></pre>

<p>migrate_from)   ocf_log info &ldquo;Migrating ${OCF_RESOURCE_INSTANCE} to ${OCF_RESKEY_CRM_meta_migrate_source}.&rdquo;<br /></p>

<pre><code>        dummy_start&lt;br /&gt;
    ;;&lt;br /&gt;
</code></pre>

<p>promote)    stateful_promote<br /></p>

<pre><code>    ;;&lt;br /&gt;
</code></pre>

<p>demote)     stateful_demote<br /></p>

<pre><code>    ;;&lt;/p&gt;
</code></pre>

<p><p>reload)      ocf_log err &ldquo;Reloading&hellip;&rdquo;<br /></p>

<pre><code>        dummy_start&lt;br /&gt;
    ;;&lt;br /&gt;
</code></pre>

<p>validate-all)   dummy_validate;;<br />
usage|help) dummy_usage<br /></p>

<pre><code>    exit $OCF_SUCCESS&lt;br /&gt;
    ;;&lt;br /&gt;
</code></pre>

<p>*)      dummy_usage<br /></p>

<pre><code>    exit $OCF_ERR_UNIMPLEMENTED&lt;br /&gt;
    ;;&lt;br /&gt;
</code></pre>

<p>esac<br />
rc=$?<br />
ocf_log debug &ldquo;${OCF_RESOURCE_INSTANCE} $__OCF_ACTION : $rc&rdquo;<br />
exit $rc
</p></p>

<p><p><strong>申明：本文中大量的代码来自<a href="http://clusterlabs.org/" target="_blank">ClusterLabs</a>， 使用时请遵守相关约束</strong></p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clusters from Scratch 实验笔记]]></title>
    <link href="http://blog.klwang.info/blog/pacemaker-cososync-cman-setup/"/>
    <updated>2013-06-06T00:00:00+08:00</updated>
    <id>http://blog.klwang.info/blog/pacemaker-cososync-cman-setup</id>
    <content type="html"><![CDATA[<p>pacemaker 经典文档 Clusters_from_Scratch 的实验笔记，留作以后参考
<h3>hosts文件设置</h3>
<pre>
# grep node /etc/hosts
192.168.15.11 node1.cluster.com node1
192.168.15.12 node2.cluster.com node2
192.168.15.13 node3.cluster.com node2
</pre></p>




<p>将hosts文件设置成类似上面的样子，并且保证各机器之间网络相通
<h3>配置ssh</h3>
<pre>
# ssh-keygen -t dsa --生成公钥文件，在所有的节点执行
# for i in {1..3}; do ssh node${i} cat /root/.ssh/id_dsa.pub >> /root/.ssh/authorized_keys; done
# for i in {1..3}; do scp /root/.ssh/authorized_keys node${i}:/root/.ssh; done
# for i in {1..3}; do ssh node${i} hostname; done
</pre></p>




<p><h3>设置主机名</h3>
<pre>
# for i in {1..3}; do ssh node${i} sed -i 's/.*//g' /etc/sysconfig/network; done
# for i in {1..3}; do ssh node${i} source /etc/sysconfig/network; done
# for i in {1..3}; do ssh node${i} hostname \$HOSTNAME; done --反斜线很重要，不然所有的机器都长一样啦
</pre></p>




<p>最终的效果：
<pre>
# hostname
node1
# dnsdomainname
cluster.com
</pre></p>




<p><h3>配置软件源</h3>
软件源的配置比较多样化，可以操作俺的另外一篇文章 <a href="http://klwang.info/centos-%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E6%BA%90%EF%BC%88redhat%EF%BC%89/">centos-使用第三方源（redhat）</a>
<h3>安装软件包</h3>
<pre>
# for i in {1..3}; do ssh node${i} yum install -y pacemaker corosync; done
</pre></p>




<p><h3>配置corosync</h3>
在/etc/corosync/corosync.conf文件中，主要调整这些参数
<pre>
mcastaddr: 226.94.1.1 --组播地址
mcastport: 4000 --组播端口(UDP比TCP小一个端口，比如目前设置就使用了tcp:4000和udp:3999)
bindnetaddr: 192.168.16.0 --最后一位是掩码,为（256 - your_mask)，比如我的掩码是255.255.240.0，则第三个点分十进制是16
rrp_mode: passive --文中的集群是3个节点（如果是两个节点的话，就可以保留rrp_mode为none）
</pre></p>




<p>为了让corosync使用pacemaker，需要配置这个文件
<pre>
# cat /etc/corosync/service.d/pcmk
service {
    # Load the Pacemaker Cluster Resource Manager
    name: pacemaker
    ver: 1
}
</pre></p>




<p>完成后，将corosync.conf和pcmk配置文件分发到所有节点上
<h3>启动集群</h3>
<pre>
# service pacemaker start
# crm_mon
Last updated: Thu Aug 27 16:54:55 2009Stack: openais
Current DC: node-1 - partition with quorum
Version: 1.1.5-bdd89e69ba545404d02445be1f3d72e6a203ba2f
3 Nodes configured, 2 expected votes
0 Resources configured.
============
Online: [ node1 node2 node3]
</pre></p>




<p><h3>关闭stonith和忽略quorum</h3>
(虽然三个节点，但是还是希望剩下一个节点的时候可以工作)
<pre>
# crm configure property no-quorum-policy=ignore
# crm configure property stonith-enabled=false
</pre></p>




<p><h3>配置第一个测试资源</h3>
(飘移ip地址)</p>




<p><pre>
# crm configure primitive ClusterIP ocf:heartbeat:IPaddr2 \
> params ip=192.168.15.200 cidr_netmask=32 \
> op monitor interval=30s timeout=60s
# crm_mon
============
Last updated: Fri Aug 28 15:23:48 2009
Stack: openais
Current DC: pcmk-1 - partition WITHOUT quorum
Version: 1.1.5-bdd89e69ba545404d02445be1f3d72e6a203ba2f
3 Nodes configured, 3 expected votes
1 Resources configured.
============</pre></p>




<p>Online: [ node1 node2 node3 ]<br />
ClusterIP (ocf::heartbeat:IPaddr): Started node1
</p>




<p><h3>添加apache服务</h3>
<pre>
# for i in {1..3}; do ssh node${i} yum install -y httpd wget; done
</pre></p>




<p><h3>配置web首页内容</h3>
<pre>
<html>
 <body>My Test Site - node2</body>
</html>
</pre></p>




<p>注意，为了测试ip地址在哪个节点，每个节点的首页文件不能相同</p>




<p>开启apache的状态监控功能
<pre>
<Location /server-status>
   SetHandler server-status
   Order deny,allow
   Deny from all
   Allow from 127.0.0.1

</pre></p>




<p><h3>添加apache的资源</h3>
<pre>
# crm configure primitive WebSite ocf:heartbeat:apache \
> params configfile=/etc/httpd/conf/httpd.conf \
> op monitor interval=1min
</pre></p>




<p><h3>设置colocation约束，将apache和ip地址绑定在一起</h3>
<pre>
# crm configure colocation website-with-ip INFINITY: WebSite ClusterIP
</pre></p>




<p><h3>设置资源的启动和停止顺序</h3>
<pre>
# crm configure order apache-after-ip mandatory: ClusterIP WebSite
</pre></p>




<p><h3>修改资源的prefer节点</h3>
<pre>
# crm configure location prefer-node1 WebSite 100: node1
</pre></p>




<p><h3>增加gfs2支持</h3>
<pre>
# for i in {1..3}; do ssh node${i} yum install -y cman gfs2-utils ccs; done
</pre></p>




<p><h3>修改cman的默认quorum等待时间</h3>
<pre>
# for i in {1..3}; do \
> do ssh node${i} sed -i.sed "s/.*CMAN_QUORUM_TIMEOUT=.*/CMAN_QUORUM_TIMEOUT=0/g" /etc/sysconfig/cman; \
> done
</pre></p>




<p><h3>建立cman的配置文件</h3>
<pre>
# ccs -f /etc/cluster/cluster.conf --createcluster cluster
# for i in {1..3}; do \
> ccs -f /etc/cluster/cluster.conf --addnode node{i}; \
> done
</pre></p>




<p>分发cluster.conf配置文件到各个节点的相应位置<br />
此时，已经不使用corosync.conf配置文件了，其功能由cluster.conf全权代替，关于 cman 和 corosync 之间的不正常关系，请参考我的另外一篇博文 <a href="http://klwang.info/cman-and-corosync/" target="_blank">cman and corosync</a>
<h3>启动集群</h3>
<pre>
service pacemaker start --此时pacemaker会自动启动cman
</pre></p>




<p><h3>建立 gfs2 文件系统资源</h3>
<pre>
# mkfs.gfs2 -p lock_dlm -j 2 -t pcmk:web /dev/drbd1
</pre></p>




<p>关于gfs文件系统的相关问题，可以参考我的另外一篇博文 <a href="http://klwang.info/gfs2-startup/" target="_blank">GFS2 初级</a></p>




<p><h3>在共享文件系统上建立主页测试文件</h3>
<pre>
# mount /dev/drbd1 /mnt/
# cat /mnt/index.html
<html>
 <body>My Test Site - GFS2</body>
</html>
# umount /mnt
</pre></p>




<p><h3>配置文件系统资源</h3>
<pre>
# configure primitive WebFS ocf:heartbeat:Filesystem \
> params device="/dev/drbd/by-res/wwwdata" directory="/var/www/html" fstype="gfs2"
# configure colocation WebSite-with-WebFS inf: WebSite WebFS
# configure order WebSite-after-WebFS inf: WebFS WebSite
</pre></p>




<p><h3>将系统设置为双active的模式</h3>
<pre>
# configure clone WebIP ClusterIP \
> meta globally-unique="true" clone-max="2" clone-node-max="2"
# configure edit ClusterIP
# crm configure clone WebFSClone WebFS
# crm configure clone WebSiteClone WebSite
</pre></p>




<p>至此，整个实验已经搭建完成，整篇博文只是记录了该如何做，没有阐述为什么这么做；关于为什么，可以参考下面的文档，一定会找到比较满意的答案</p>




<p>参考文章
<a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/index.html" target="_blank">Clusters from Scratch</a>
<a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Pacemaker_Explained/index.html" target="_blank">Configuration Explained</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GFS2 初级]]></title>
    <link href="http://blog.klwang.info/blog/gfs2-startup/"/>
    <updated>2013-06-05T00:00:00+08:00</updated>
    <id>http://blog.klwang.info/blog/gfs2-startup</id>
    <content type="html"><![CDATA[<p><h4>建立文件系统</h4>
<pre># mkfs.gfs2 -p lock_dlm -t alpha:mydata1 -j 8 /dev/vg01/lvol0</pre>
-p 指定要使用的锁定协议名称，集群的锁定协议为 lock_dlm<br />
-t 这个参数是用来指定集群配置中的 GFS2 文件系统。它有两部分：ClusterName:FSName
<p style="padding-left: 30px;">ClusterName，用来创建 GFS2 文件系统的集群名称<br />
FSName，文件系统名称</p>
-j 指定由 mkfs.gfs2 命令生成的日志数目
<h4>挂载文件系统</h4>
<pre># mount -t gfs2 -o noatime /dev/mapper/mpathap1 /mnt</pre>
使用 noatime 可以避免gfs2在读取文件时更新文件的access时间戳，进而实现读写分离<br />
如果在未启动集群时想单机挂载文件系统，可以使用 lockproto=lock_nolock 参数
<h4>卸载文件系统</h4>
<pre># umount /mnt</pre>
没啥好说的
<h4>配额管理</h4>
挂载的时候使用 quota 选项即可
<pre># mount -o quota=on /dev/vg01/lvol0 /mnt
# quotacheck -ug /mnt                          --创建配额数据库文件
# edquota username                             --设置用户的配额
# quota username                               --验证用户的配额
# edquota -g devel                             --设置组的配额
# quota -g devel                               --验证组的配额</pre>
<h4>容量管理</h4>
<pre># gfs2_grow /mnt</pre>
前提：扩容之前一定要保证gfs2所在的是逻辑卷，并且已经被增大(使用： lvextend)<br />
注意：gfs2只能增大，不能缩小！
<h4>添加日志文件</h4>
gfs2 系统在集群中挂载时，每个节点对应一个日志文件，如果要加入新的节点，就需要添加日志文件
<pre># gfs2_tool journals /mnt                  --察看现在的日志数量
# gfs2_jadd -j1 /mnt                            --添加一个日志文件
</pre>
<h4>挂起gfs2的写入操作</h4>
挂起gfs2的写入操作，可以给管理员提供拷贝文件的机会
<pre>
# dmsetup suspend /mnt                          --挂起写入操作
# dmsetup resume /mnt                           --恢复
</pre></p>




<p><h4>修复文件系统</h4>
<pre>
# fsck.gfs2 -y /mnt
</pre></p>




<p><h4>绑定挂载</h4>
gfs2不支持软链接，可以使用bind多处挂载代替
<pre>
# mount --bind olddir newdir
</pre></p>




<p>参考文档
<a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Global_File_System_2/index.html" title="Global_File_System_2">RedHat_Global_File_System_2</a>
<a href="http://sourceware.org/cluster/doc/usage.txt">gfs2_usage</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[cman and corosync]]></title>
    <link href="http://blog.klwang.info/blog/cman-and-corosync/"/>
    <updated>2013-06-04T00:00:00+08:00</updated>
    <id>http://blog.klwang.info/blog/cman-and-corosync</id>
    <content type="html"><![CDATA[<p>在pacemaker的经典文档<a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/index.html" target="_blank">Clusters from Scratch</a> 中，使用到了cman 这个红帽cluster套件中的工具；</p>




<p>奇怪的是，一开始使用的是corosync作为集群的通信层，而到了gfs作为文件系统的active/active时却换成了cman；</p>




<p>同时，corosync.conf中配置的参数貌似也都不起作用了，想要配置两条心跳线也不知道要怎么搞了；</p>




<p>corosync和cman到底是什么关系，到底能不能配置多条心跳线呢？
<h3>cman和corosync的关系</h3>
<p style="padding-left: 30px;">在cman的配置文件cluster.conf中只需要提供集群中节点的机器名就可以了，不需要设置心跳地址；</p>
<p style="padding-left: 30px;">cman会取出$HOSTNAME，然后在cluster.conf中寻找对应的信息（找不到就把dnsdomainname去掉然后再匹配；实在不行，就取出机器的ip地址，然后再去cluster.conf文件中匹配）</p>
<p style="padding-left: 30px;">当前，在寻找心跳ip的过程中，hosts文件起了至关重要的作用</p>
<p style="padding-left: 30px;">cman会自动生成multicast地址，端口号，还有其他的totem参数</p>
<p style="padding-left: 30px;">如果想要知道cman生成corosync参数是什么，可以执行下面的命令</p>
<pre>
    # corosync-objctl -a|grep ^totem
</pre>
<p style="padding-left: 30px;">cman首先找到机器的心跳ip地址，并设置好muliticast信息</p>
<pre>
    totem.interface.ringnumber=0
    totem.interface.bindnetaddr=192.168.1.29
    totem.interface.mcastaddr=239.192.209.5
    totem.interface.mcastport=5405
</pre>
<p style="padding-left: 30px;">然后，cman会将下面的这些参数设置的比一般的corosync集群稍微高一些</p>
<pre>
    totem.token=10000
    totem.join=60
    totem.fail_recv_const=2500
    totem.consensus=12000
</pre>
<p style="padding-left: 30px;">下来，cman开启通信加密（根据节点的数量，设置rrp_mode的值）</p>
<pre>
    totem.rrp_mode=none
    totem.secauth=1
    totem.key=composers
</pre>
<p style="padding-left: 30px;">最后，cman将集群的quorum提供者设置成它自己</p>
<pre>
    quorum.provider=quorum_cman
</pre></p>




<p><h3>设置多条心跳线</h3>
<p style="padding-left: 30px;">cman的第一条心跳线是根据hosts文件取出的（当然，也可以在clusternode的name中直接写ip地址）</p>
<p style="padding-left: 30px;">比如：</p>
<pre>
    <clusternode name="LIN01-adm" nodeid="1" votes="1" />
</pre>
<p style="padding-left: 30px;">cman会从host文件中取LIN01-adm对应的ip地址作为心跳地址；</p>
<p style="padding-left: 30px;">需要添加多条心跳线时，就要增加altername选项了</p>
<p style="padding-left: 30px;">比如：</p></p>




<p><pre>
    <clusternode name="cent1" nodeid="1">
      <altname name="192.148.1.1" />
    </clusternode>
</pre>
<p style="padding-left: 30px;">这样，就多了一条192.168.1.1的心跳线了(当然也可以写名字，然后cman自动从hosts文件中解析地址)</p></p>




<p><h3>相关工具的使用</h3>
<p style="padding-left: 30px;">既然使用了cman，就索性把红帽的其他工具也好好用一下<br />
ccs-红帽的集群配置工具（专门用来写cluster.conf文件的）</p></p>




<p><p style="padding-left: 30px;">建立集群</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --createcluster cluster_name
</pre>
<p style="padding-left: 30px;">添加节点</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --addnode nodename
</pre>
<p style="padding-left: 30px;">删除节点</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --rmnode nodename
</pre>
<p style="padding-left: 30px;">添加备用名（备用心跳）</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --addalt nodename alt_name
</pre>
<p style="padding-left: 30px;">删除备用名</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --rmalt nodename alt_name
</pre>
<p style="padding-left: 30px;">其他命令 man ccs 即可</p>
    cman_tool
<p style="padding-left: 30px;">写好了 custer.conf 文件，我们就需要在集群中发布一下，也很简单</p>
<p style="padding-left: 30px;">前提条件，设置好 ricci 用户的密码，开启 ricci 服务</p>
<pre>
    service ricci start &amp;&amp; chkconfig ricci on
    password ricci
</pre>
<p style="padding-left: 30px;">分发配置文件</p>
<pre>
    cman_tool versiom -r
</pre></p>




<p><p style="padding-left: 30px;">其他工具，大家自己探索吧</p></p>




<p>参考文档
<a href="http://chrissie.fedorapeople.org/CmanYinYang.pdf" title="CmanYinYang">CmanYinYang</a>
<a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html-single/Cluster_Administration/index.html" title="RedHat_Cluster_Administration" target="_blank">RedHat_Cluster_Administration</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kvm 虚拟机 fence 设置]]></title>
    <link href="http://blog.klwang.info/blog/kvm-guest-fence/"/>
    <updated>2013-06-03T00:00:00+08:00</updated>
    <id>http://blog.klwang.info/blog/kvm-guest-fence</id>
    <content type="html"><![CDATA[<p>习惯于使用 虚拟机 测试HA配置文件，但是对于fence这块，一直没有办法搞定</p>




<p>之前看到有人用external/ssh的，就想试试，但是后知后觉的发现，这玩意竟然被pacemaker给枪毙了（<a href="http://hg.linux-ha.org/glue/rev/5ef3f9370458">相关链接</a>）官方说来说去就是一句话来解释这个问题(No. external/ssh simply cannot be relied on)
<pre>
>> You really don't want to rely on SSH STONITH in a production environment.
>>
>> Regards,
>>
>> Tim
>>
>> Sure, but I'm in a lab environment at the moment without
>> UPS-based STONITH capabilities, so having SSH STONITH working
>> to test things out would be helpful.</pre></p>




<p>The development package should contain external/ssh.</p>




<p>> Also, even in production,<br />
> what would be the harm of having both SSH and UPS-based STONITH<br />
> available? Wouldn't more routes to STONITH a node be better?</p>




<p>No. external/ssh simply cannot be relied on.
算了，不给用就不用好了，kvm 反正还有其他的方式来实现fence呢
<h3>host机操作</h3>
<h4>1 把包先装上</h4>
<pre># yum install -y fence-virt fence-virtd fence-virtd-libvirt fence-virtd-multicast</pre>
<h4>2 搞一个认证文件</h4>
没有啥限制，主要是内部通信用，通信机器之间文件一致即可
<pre># mkdir /etc/cluster
# dd if=/dev/urandom of=/etc/cluster/fence_xvm.key bs=4096 count=1</pre>
<h4>3. 生成配置文件</h4>
<pre># fence_virtd -c</pre>
有一点需要改改，其他的按照提示一路回车就ok
<pre># Backend module [checkpoint]: libvirt  --因为我们只安装了fence-virtd-libvirt这个backend</pre>
<h4>4.配置文件review</h4>
生成的配置文件长这个样 /etc/fence_virt.conf
<pre>backends {
    libvirt {
        uri = "qemu:///system";
    }</pre></p>




<p>}</p>




<p>listeners {<br />
    multicast {<br />
        interface = "br0";<br />
        port = "1229";<br />
        family = "ipv4";<br />
        address = "225.0.0.12";<br />
        key_file = "/etc/cluster/fence_xvm.key";<br />
    }</p>




<p>}</p>




<p>fence_virtd {<br />
    module_path = "/usr/lib64/fence-virt";<br />
    backend = "libvirt";<br />
    listener = "multicast";<br />
}
就是一般的配置文件，对于不明白的地方，man fence_virt.conf 即可找到喜欢的解释
<strong><em>特别注意： interface = "br0"; 这个参数一定要仔细对待，不要被它的默认提示给忽悠了，这个监听的网卡一定要选择一个host机和guest机能够相互通信的网卡（像virtbr0这种kvm默认的网卡要仔细喽），比如，如果你的guest机是bridge的，那就写host的对外网卡就ok了</em></strong>
<h4><strong><em></em></strong> 5.启动服务</h4>
<pre># service fence_virtd start
# chkconfig --add fence_virtd
# chkconfig fence_virtd on</pre>
<h4>6. 验证配置</h4>
<pre># fence_xvm -o list</pre></p>




<p>cluster_node1             f2bd9d70-411e-393c-0720-3311985a63bf on<br />
cluster_node2             3fd4a9cd-aa43-11b1-a943-9f8623b790b3 on<br />
cluster_node3             f2bd9d70-411e-393c-0720-3314335a34bf on<br />
cluster_node4             3fd4a9cd-aa43-11b1-a943-9f234b7934b3 on
重启一个guest机试试？（不要说我没告诉你-o reboot是重启的意思哦）
<pre># fence_xvm -o reboot -H cluster_node2</pre>
<h3>guest机配置</h3>
<h4>1. 安装软件包</h4>
这个 fence-virt 就是为了试试 fence_xvm 命令的，其实不装也ok的
<pre># yum install -y fence-virt</pre>
<h4>2. 同步配置文件</h4>
把之前在host上生成的 /etc/cluster/fence_xvm.key 文件拷贝到所有guest的对应位置
<h4>3. 测试</h4>
<pre># fence_xvm -o list</pre></p>




<p>cluster_node1             f2bd9d70-411e-393c-0720-3311985a63bf on<br />
cluster_node2             3fd4a9cd-aa43-11b1-a943-9f8623b790b3 on<br />
cluster_node3             f2bd9d70-411e-393c-0720-3314335a34bf on<br />
cluster_node4             3fd4a9cd-aa43-11b1-a943-9f234b7934b3 on
同样，可以看到其它的机器，<br />
试试？别把自己fence掉啦
<pre># fence_xvm -o reboot -H cluster_node2</pre>
ok啦，可以使用这个fence设备了
<pre> crm configure primitive st-virt stonith:fence_xvm \
 params port="cluster_node1 cluster_node2 cluster_node3 cluster_node4"</pre>
这个port就是fence机器的列表</p>




<p>&nbsp;</p>




<p>参考文章
<a href="http://www.daemonzone.net/e/3/" target="_blank">Guest fencing on a RHEL KVM host</a>
<a href="http://clusterlabs.org/wiki/Guest_Fencing" target="_blank">Guest Fencing</a></p>

]]></content>
  </entry>
  
</feed>
