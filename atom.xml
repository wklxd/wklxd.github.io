<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[My Octopress Blog]]></title>
  <link href="http://github.klwang.info/atom.xml" rel="self"/>
  <link href="http://github.klwang.info/"/>
  <updated>2013-06-23T00:01:53+08:00</updated>
  <id>http://github.klwang.info/</id>
  <author>
    <name><![CDATA[wklxd]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[About strace]]></title>
    <link href="http://github.klwang.info/blog/about-strace/"/>
    <updated>2013-06-21T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/about-strace</id>
    <content type="html"><![CDATA[<p>作为软件的使用者而非开发者，平时会遇到很多奇怪的问题</p>

<p>这个程序为什么不动呢，它是不是僵死了？它为什么没有按照预想的方式工作？</p>

<p>时至今日，应用程序已经完全和硬件隔离，具体的操作（文件读写，网络访问等）都要使用 syscalls（系统调用）来完成</p>

<p>如果可以跟踪程序使用的syscall，就意味着几乎知道了程序的所有动作，比如</p>

<p><pre>
    open -> read/write						读写文件
    socket -> connect -> read -> write		                访问网络
    fork -> execve						衍生子进程
    wait, kill, pipe						控制子进程
    clock_gettime						获知系统时间
</pre></p>

<p>这时，我们可以寻求系统自带的调试利器 strace 的帮助</p>

<p><pre>
    strace	跟踪process系统调用或者信号产生的情况
    ltrace	跟踪process调用库函数的情况
</pre></p>

<p>strace和ltrace的使用方式差不多，只是跟踪的范围不一样</p>

<p>0. 基本使用<br />
	
<pre>
    strace command args
</pre></p>

<p>直接使用strace调用具体的程序，会显示程序运行过程中的所有syscall</p>

<p><strong>1. 看看程序正在干什么</strong></p>

<p><pre>
    strace -p pid
</pre></p>

<p>除了基本的使用方式，strace还可以在线调试运行中的程序</p>

<p><em>这时，就能够很容易的知道为啥这个程序会不动呢？ 它是在等待别的资源，还是在勤劳的干活？</em></p>

<p><strong>2. 看看程序在启动的时候打开了什么文件</strong></p>

<p><pre>
    [root@h4-61 tmp]# strace -e read,access ping -c 1 8.8.8.8 > /dev/null
    access("/etc/ld.so.preload", R_OK)      = -1 ENOENT (No such file or directory)
    read(3, "\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0) = 832
    read(3, "\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0>\0\1\0) = 832
</pre></p>

<p>strace默认会显示所有的系统调用，有时。我们可能只关心少量的syscall，可以使用 -e 参数来实现</p>

<p>多个syscall使用逗号(,)分割；甚至，可以使用 read=3 来查看使用 3 这个描述符的 read 调用</p>

<p><strong>3. 显示syscall的时间戳</strong></p>

<p><pre>
    [root@h4-61 tmp]# strace -tt -e read,access ping -c 1 8.8.8.8 > /dev/null
    17:59:05.205188 access("/etc/ld.) = -1 ENOENT (No such file or directory)
    17:59:05.205660 read(3, "\177ELF\2\1\) = 832
    17:59:05.205972 read(3, "\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0>\0) = 832
</pre></p>

<p>有时需要知道syscall发生的时间，可以使用 -t 参数：</p>

<p><em>-t  表示秒</em></p>

<p><em>-tt 表示毫秒</em></p>

<p><em>-T  显示每个系统调用从开始到结束使用的时间</em></p>

<p><strong>4. 控制syscall显示信息的程度</strong></p>

<p><pre>
    [root@h4-61 tmp]# strace -tt -e read,access -s 10 ping -c 1 8.8.8.8 > /dev/null
    17:59:42.831239 access("/etc/ld.) = -1 ENOENT (No such file or directory)
    17:59:42.831660 read(3, "\177ELF\2\1\1\0\0\0"..., 832) = 832
    17:59:42.831967 read(3, "\177ELF\2\1\1\3\0\0"..., 832) = 832
</pre></p>

<p>默认情况下，strace对于每个系统调用只显示32个字符的信息，使用 -s 数字可以改变默认行为</p>

<p>-s 0 表示显示所有的信息</p>

<p><strong>5. 分类查看syscall</strong></p>

<p>除了使用 -e syscallname 查看具体的syscall，我们还可以按照类别产看，如</p>

<p><strong>trace=file</strong></p>

<p><em>显示文件操作	open,stat,chmod,unlink,..</em></p>

<p><pre>
    [root@h4-61 tmp]# strace -e trace=file ping -c 1 8.8.8.8 > /dev/null
    execve("/bin/ping", ["ping", "-c", "1", "8.8.8.8"], [/* 26 vars */]) = 0
    access("/etc/ld.so.prel)      = -1 ENOENT (No such file or directory)
    open("/etc/ld.so.cache", O_RDONLY)      = 3
    open("/lib64/libidn.so.11", O_RDONLY)   = 3
    open("/lib64/libc.so.6", O_RDONLY)      = 3
    open("/usr/lib/locale/locale-archive", O_RDONLY) = 3
</pre></p>

<p><strong>trace=process</strong></p>

<p><em>显示进程操作 fork, wait, and exec </em></p>

<p><pre>
    [root@h4-61 tmp]# strace -e trace=process ping -c 1 8.8.8.8 > /dev/null
    execve("/bin/ping", ["ping", "-c", "1", "8.8) = 0
    arch_prctl(ARCH_SET_FS, 0x7fc9afee4700) = 0
    exit_group(0)  = ?
</pre>
             
<strong>trace=network</strong></p>

<p><em>网络操作相关的syscall</em></p>

<p><pre>
    [root@h4-61 tmp]# strace -e trace=network ping -c 1 8.8.8.8 > /dev/null
    socket(PF_INET, SOCK_RAW, IPPROTO_ICMP) = 3
    socket(PF_INET, SOCK_DGRAM, IPPROTO_IP) = 4
    省略号
    setsockopt(3, SOL_SOCKET, SO_SNDBUF, [324], 4) = 0
    setsockopt(3, SOL_SOCKET, SO_RCVBUF, [65536], 4) = 0
    getsockopt(3, SOL_SOCKET, SO_RCVBUF, [3427721778695372800], [4]) = 0
    setsockopt(3, SOL_SOCKET, SO_TIMESTAMP, [1], 4) = 0
    省略号
</pre></p>

<p><strong>trace=signal</strong></p>

<p><em>信号操作相关的syscall</em></p>

<p><pre>
    [root@h4-61 tmp]# strace -e trace=signal ping -c 1 8.8.8.8 > /dev/null
    rt_sigaction(SIGINT, {0x7f8f4d04da40, [], SA_RESTORER|SA_IN) = 0
    rt_sigaction(SIGALRM, {0x7f8f4d04da40, [], SA_RESTORER|SA_I) = 0
    rt_sigaction(SIGQUIT, {0x7f8f4d04da50, [], SA_RESTORER|SA_INT) = 0
</pre></p>

<p><strong>trace=ipc</strong></p>

<p><em>进程间通信相关的syscall</em></p>

<p><strong>trace=desc</strong></p>

<p><em>文件描述符相关的syscall</em></p>

<p><pre>
    [root@h4-61 tmp]# strace -e trace=desc ping -c 1 8.8.8.8 > /dev/null
    open("/etc/ld.so.cache", O_RDONLY)      = 3
    fstat(3, {st_mode=S_IFREG|0644, st_size=61926, ...}) = 0
    close(3)                                = 0
    open("/lib64/libidn.so.11", O_RDONLY)   = 3
    read(3, "\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0\1) = 832
    省略号
    write(1, "\n", 1)                       = 1
    write(1, "--- 8.8.8.8 ping statistics ---\n"..., 146) = 146
</pre></p>

<p><strong>6. 跟踪子进程</strong></p>

<p><pre>
    -f 
</pre></p>

<p>有时候程序会fork出子进程，使用 -f 参数可以跟踪到子进程</p>

<p><strong>7. strace的输出格式</strong>
	
<pre>
    fstat(3, {st_mode=S_IFREG|0644, st_size=61926, ...}) = 0
</pre></p>

<p>每行的syacall格式为， syscall名字，syscall的参数， syscall的返回值</p>

<p><em>如果返回值为 -1 ，则顺带的返回具体的错误说明</em></p>

<p>To be continued &#8230;</p>

<p>参考文档</p>

<p><a href="https://blogs.oracle.com/ksplice/entry/strace_the_sysadmin_s_microscope" target="_blank">strace_the_sysadmin_s_microscope</a></p>

<p><a href="http://www.hokstad.com/5-simple-ways-to-troubleshoot-using-strace" target="_blank">5-simple-ways-to-troubleshoot-using-strace</a></p>

<p><a href="http://linux.die.net/man/1/strace" target="_blank">strace man page</a>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ubuntu NO_PUBKEY 故障解决方式]]></title>
    <link href="http://github.klwang.info/blog/about-ubuntu-no-pubkey-error/"/>
    <updated>2013-06-19T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/about-ubuntu-no-pubkey-error</id>
    <content type="html"><![CDATA[<p>ubuntu在更新源后出现</p>

<p><pre>
    W: GPG 错误：http://ppa.launchpad.net hardy Release: 
    由于没有公钥，无法验证下列签名： NO_PUBKEY 4B82DCA0798F627E
</pre></p>

<p>原因是对应的公钥没有导入</p>

<p>公钥服务器有很多个，常用的有</p>

<p><pre>
    subkeys.pgp.net
    wwwkeys.pgp.net
</pre></p>

<p>如果一个key server找不到需要的公钥，可以考虑换服务器试试</p>

<p>最终的处理办法</p>

<p><pre>
    gpg --keyserver subkeys.pgp.net --recv 798F627E  （4B82DCA0798F627E的后八位）
    gpg --export --armor 798F627E | sudo apt-key add -
</pre></p>

<p>公钥导入后，就可以正常使用源了</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[你不知道的bash (特殊字符)]]></title>
    <link href="http://github.klwang.info/blog/bash-you-don-not-konw-special-character/"/>
    <updated>2013-06-17T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/bash-you-don-not-konw-special-character</id>
    <content type="html"><![CDATA[<p><strong>题记</strong>
<pre>
  使用Linux有四年多了
  曾经有段时间，也静下心来将bash好好的研究了一把，心满意足的感觉自己“会”了
  随着时间的推移，常用的命令就那么几个，渐渐的，知识面逐渐收缩
  再次看到bash资料时，才发现：“原来可以不用这么绕，原来可以这么直接”
  学习就是一个温故知新的过程，技术就是一个在使用过程中逐渐积累的过程
  这篇文档题目为《你不知道的bash》，其实，更确切的讲，应该是《我不知道的bash》
  在”写下来“的过程中，不仅仅梳理了忘记的东西，还顺带的整理了一些bash中的经典用法
  希望以后不要再忘了
</pre></p>

<p><strong>;; </strong></p>

<p><em>双分号,作为case语句的结束符使用</em>，如：
<pre>
  case "$variable" in
     abc)  echo "\$variable = abc" ;;
     xyz)  echo "\$variable = xyz" ;;
  esac
</pre></p>

<p><strong>**</strong></p>

<p><em>双星号，(立方)</em>，如：
<pre>
  # Bash, version 2.02, introduced the "**" exponentiation operator.
     
  let "z=5**3"
  echo "z = $z"   # z = 125
</pre></p>

<p><strong>$?</strong> </p>

<p><em>结束符，可以是一条命令，一个函数，或者脚本自身的退出码</em>
<pre>
  cmd
  if [ $? ne 0 ]; then
      exit 2
  fi
</pre></p>

<p><strong>$$</strong> </p>

<p><em>进程号，表示脚本自身的进程号，经常看到有人使用$$作为随机数</em>，如：
<pre>
  cmd
  ramdom=$$
</pre></p>

<p><strong>$!</strong> </p>

<p><em>上一条命令的pid，经典用法，杀掉超时的前一个作业</em>
<pre>
  possibly_hanging_job & { sleep ${TIMEOUT}; eval 'kill -9 $!' &> /dev/null; }
</pre></p>

<p><strong>$_</strong></p>

<p><em>上一条命令的参数</em></p>

<p><strong>{xxx,yyy,zzz,&#8230;}</strong></p>

<p><em>加强扩展</em>，如
<pre>
  cat {file1,file2,file3} > combined_file
  # 将 file1, file2, 和 file3 合并入 combined_file
     
  cp file22.{txt,backup}
  # 将 "file22.txt" 复制为 "file22.backup"
</pre></p>

<p><strong>=~</strong></p>

<p><em>文本判断时的正则表达式</em>， 如：
<pre>
  if [ "$input" =~ "[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]" ]
      # 引号可要可不要
      # 格式 NNN-NN-NNNN
  then
      echo "Social Security number."
      # Process SSN.
  else
      echo "Not a Social Security number!"
      # Or, ask for corrected input.
  fi
</pre></p>

<p><strong>&></strong></p>

<p><em>重定向，是不是经常使用这种语句</em>：
<pre>
  command > filename 2 >&1
</pre>
其实你还可以这样，将标准输出和错误输出同时定向
<pre>
  command &>filename 
</pre></p>

<p><strong>$*</strong></p>

<p><em>所有的位置参数，看作一个整体来使用，具体操作时 $* 需要用引号(&#8220;)引起来</em></p>

<p><strong>$@</strong></p>

<p><em>和 $* 相同，但是在具体操作的时候，bash将每个位置参数分开来看</em>，如:
<pre>
  #!/bin/bash</pre></p>

<p>  echo &#8216;$*&#8217;<br />
  index=1</p>

<p>  for arg in &#8220;$*&#8221; <br />
  do<br />
    echo &#8220;Arg #$index = $arg&#8221;<br />
    let &#8220;index+=1&#8221;<br />
  done</p>

<p>  echo ‘$@’<br />
  index=1</p>

<p>  for arg in &#8220;$@&#8221;<br />
  do<br />
    echo &#8220;Arg #$index = $arg&#8221;<br />
    let &#8220;index+=1&#8221;<br />
  done 
</p>

<p>执行结果如下， 可以看到 $*将所有的参数看作了整体， $@将参数分开对待了：
<pre>
  ./test 1 2 3 4
  $*
  Arg #1 = 1 2 3 4
  $@
  Arg #1 = 1
  Arg #2 = 2
  Arg #3 = 3
  Arg #4 = 4
</pre></p>

<p>to be continue &#8230;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[lvm 实践笔记]]></title>
    <link href="http://github.klwang.info/blog/about-lvm-skills/"/>
    <updated>2013-06-12T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/about-lvm-skills</id>
    <content type="html"><![CDATA[<p><strong>物理卷</strong></p>

<p>创建物理卷
<pre>
# pvcreate /dev/sdd /dev/sde /dev/sdf
</pre>
  <br />
察看已经存在的物理卷信息</p>

<p><pre>
# pvs				--每行一条，便于脚本调用
# pvdisplay			--显示pv的详细信息
</pre></p>

<p>阻止在物理卷上分配空间 （比如，某个物理磁盘发生了硬件故障，需要移除这个硬盘的时候）</p>

<p><pre>
# pvchange -x n /dev/sdk1
</pre></p>

<p>重新允许在物理卷上分配空间</p>

<p><pre>
# pvchange -x y /dev/sdk1
</pre></p>

<p>移除物理卷（前提，没有卷组使用该pv）     </p>

<p><pre>
# pvremove /dev/ram15
  Labels on physical volume "/dev/ram15" successfully wiped
</pre></p>

<p>替换/移除物理卷</p>

<p><pre>
# pvmove /dev/sdc1 SourcePhysicalVolume     --从SourcePhysicalVolume中移除sdc1
</pre></p>

<p>只移除 sdc1 中 MyLV 逻辑卷上的的数据</p>

<p><pre>
# pvmove -n MyLV /dev/sdc1
</pre></p>

<p><strong>卷组</strong></p>

<p>创建卷组</p>

<p><pre>
# vgcreate vg1 /dev/sdd1 /dev/sde1
</pre></p>

<p>给卷组中加入新的物理卷</p>

<p><pre>
# vgextend vg1 /dev/sdf1
</pre></p>

<p>察看已经存在的卷组信息</p>

<p><pre>
# vgs		--每行一条，便于脚本调用
# vgdisplay	--现实vg卷组的详细信息
# vgscan	--重建 /etc/lvm/cache/.cache file cache文件，维护目前的 LVM 设备列表
</pre></p>

<p>系统启动时，会自动运行vgscan命令。如果集群中在一个节点进行了 vgextend 等命令，在其他节点手动运行 vgscan 将是很好的选择</p>

<p>移动物理卷上的数据（适用于替换硬盘时）</p>

<p><pre>
# vgreduce my_volume_group /dev/hda1
</pre>
该命令会将 /dev/hda1 上的数据挪到同卷组内的其他物理卷上</p>

<p>限制卷组中最大的逻辑卷数量</p>

<p><pre>
# vgchange -l 128 /dev/vg00
</pre></p>

<p>将卷组改成闲置状态（维护时使用）</p>

<p><pre>
# vgchange -a n my_volume_group
</pre></p>

<p>重新激活卷组</p>

<p><pre>
# vgchange -a y my_volume_group
</pre></p>

<p>移除卷组（要保证移除的卷组上的逻辑卷都已经移除了）</p>

<p><pre>
# vgremove officevg
  Volume group "officevg" successfully removed
</pre></p>

<p>卷组拆分（将/dev/ram15 从 bigvg 中拆出来组成名为 smallvg 的卷组）</p>

<p><pre>
# vgsplit bigvg smallvg /dev/ram15
  Volume group "smallvg" successfully split from "bigvg"
</pre></p>

<p>卷组合并（前提条件：两个卷组的 extent sizes 必须相同）</p>

<p><pre>
# vgmerge -v databases my_vg
</pre></p>

<p>将 my_vg 合并到 database卷组, -v表示显示详细信息，其中 my_vg 必须处于 inactive 状态， database可以处于active或者inactive状态</p>

<p>卷组改名</p>

<p><pre>
# vgrename /dev/vg02 /dev/my_volume_group
</pre></p>

<p><strong>逻辑卷</strong></p>

<p>创建逻辑卷 </p>

<p><pre>
# lvcreate -L1500 -n testlv testvg   		--1500M 默认单位M
# lvcreate -l 60%VG -n mylv testvg   		--使用卷组的60%
# lvcreate -l 100%FREE -n yourlv testvg 	--使用testvg的全部剩余空间
</pre></p>

<p>创建条带化的逻辑卷</p>

<p><pre>
# lvcreate -L 50G -i2 -I64 -n gfslv vg0		-i表示设备的数量， -I表示条带的大小（KB）
</pre></p>

<p>制定条带化时的详细信息</p>

<p><pre>
# lvcreate -l 100 -i2 -nstripelv testvg /dev/sda1:0-49 /dev/sdb1:50-99
  Using default stripesize 64.00 KB
  Logical volume "stripelv" created
</pre>
表示大小为 100个PE， 其中，使用 sda1 的0-49和 sda2 的 50-99</p>

<p>创建镜像化的逻辑卷</p>

<p><pre>
# lvcreate -L 50G -m1 -n mirrorlv vg0		-- -m表示镜像（复制）的数量
</pre></p>

<p>镜像逻辑卷按照extend来拷贝，默认是512KB，可以使用 -R num M 的方式进行调整</p>

<p>注意：超过1.5T的就不能使用 512K了</p>

<p><strong>拇指定律： 使用2的指数， 3-4T：4M，   5-8T： 8M</strong></p>

<p>使用快照</p>

<p><pre>
# lvcreate --size 100M --snapshot --name snap /dev/vg00/lvol1
</pre></p>

<p>为 /dev/vg00/lvol1 创建名为 snap 的快照，可以用于备份<br />
 
使用merge操作</p>

<p><pre>
# lvconvert --merge vg00/lvol1_snap
</pre></p>

<p>将 lvol1_snap merge回源逻辑卷，回复备份时可以考虑此操作<br />
 
收缩逻辑卷</p>

<p><pre>
# lvreduce -l -3 vg00/lvol1
</pre>
将逻辑卷减少3个PE</p>

<p>修改逻辑卷的权限</p>

<p><pre>
# lvchange -pr vg00/lvol1      --将逻辑卷改为只读模式
# lvchange -prw vg00/lvol1     --将逻辑卷修改为读写模式
</pre></p>

<p>逻辑卷改名</p>

<p><pre>
# lvrename /dev/vg02/lvold /dev/vg02/lvnew
</pre></p>

<p>删除逻辑卷</p>

<p><pre>
# lvremove /dev/vg02/lvol1
</pre></p>

<p>增大逻辑卷</p>

<p>增大到 12G</p>

<p><pre>
# lvextend -L12G /dev/myvg/homevol
lvextend -- extending logical volume "/dev/myvg/homevol" to 12 GB
lvextend -- doing automatic backup of volume group "myvg"
lvextend -- logical volume "/dev/myvg/homevol" successfully extended
</pre></p>

<p>增大1G</p>

<p><pre>
# lvextend -L+1G /dev/myvg/homevol
lvextend -- extending logical volume "/dev/myvg/homevol" to 13 GB
lvextend -- doing automatic backup of volume group "myvg"
lvextend -- logical volume "/dev/myvg/homevol" successfully extended
</pre></p>

<p>将卷组中所有的空闲附加到已经存在的逻辑卷</p>

<p><pre>
lvextend -l +100%FREE /dev/myvg/testlv
  Extending logical volume testlv to 68.59 GB
  Logical volume testlv successfully resized
</pre></p>

<p>收缩逻辑卷</p>

<p><pre>
# lvreduce -l -3 vg00/lvol1
</pre>
收缩3个PE，在收缩卷之前要保证先将里边的文件系统进行收缩，不然会丢失数据</p>

<p>如果数据量较大，需要使用-n参数后台进行，下面的命令表示将 sdc1 上的数据移动到 sdf1</p>

<p><pre>
# pvmove -b /dev/sdc1 /dev/sdf1
</pre></p>

<p><strong>综合操作实例（跨系统的卷组迁移）</strong></p>

<p>1. 卸载卷组上的所有逻辑卷</p>

<p><pre>
umount 
</pre></p>

<p>2. 将卷组改成闲置状态</p>

<p><pre>
vgchange -a n olumeGroupName
</pre></p>

<p>3. 导出VolumeGroupName的信息</p>

<p><pre>
vgexport VolumeGroupName
</pre></p>

<p>4. 做一次重建cache扫描</p>

<p><pre>
vgscan
</pre></p>

<p>5. 察看所有相关物理卷的状态，一定要处于export状态</p>

<p><pre>
pvscan
    PV /dev/sda1
    is in exported VG myvg [17.15 GB / 7.15 GB free]
    PV /dev/sdc1
    is in exported VG myvg [17.15 GB / 15.15 GB free]
    PV /dev/sdd1
    is in exported VG myvg [17.15 GB / 15.15 GB free]
    ...
</pre></p>

<p>6. 拔下硬盘装到新的机器</p>

<p>7. 导入VolumeGroupName的信息</p>

<p><pre>
vgimport VolumeGroupName
</pre></p>

<p>8. 激活卷组</p>

<p><pre>
vgchange -a y VolumeGroupName
</pre></p>

<p>9. 重新挂在相关逻辑卷</p>

<p><pre>
mount
</pre></p>

<p>That&#8217;s all</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[将 resouce angent 改造成 multi state 类型]]></title>
    <link href="http://github.klwang.info/blog/change-your-resouce-angent-to-multistate/"/>
    <updated>2013-06-10T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/change-your-resouce-angent-to-multistate</id>
    <content type="html"><![CDATA[<p>习惯了资源clone的方便快捷</p>

<p>突然发现，有时候我们需要在这些无差异的clone资源中选出一个来，好做一些“不一样“的事情，</p>

<p>就好像是在一群人中选出一个代表，来做一些领导性的工作一样</p>

<p>怎么样来做到这点呢？</p>

<p>pacemaker提供了多态(multi-state)的概念，除了具备clone的特性外，还多出了</p>

<p>master/slave的概念，可以在资源agent中判断自己所处的角色，然后根据角色的不同，做一些不同的操作</p>

<p>那么，如何将自己的agent改造为 master/slave 的呢？</p>

<p>最简单的思路：在每个节点写一个文件，记录下自己所处的角色好了</p>

<p>有了思路就可以开始实施了，这里完全照搬 pacemaker/Stateful 资源的做法</p>

<p>改造开始</p>

<p>1. 设置 $CRM_MASTER</p>

<p><pre>
    在 # Initialization: 块中加入
    CRM_MASTER="${HA_SBIN_DIR}/crm_master -l reboot"
</pre></p>

<p>2. usage 函数改造[可选]</p>

<p>usage函数本来就是给“人”看的，就算不修改，也不会影响功能。可以加入以下的内容对demote和promote进行说明
<pre>
    The 'promote' operation xxxxxx xxxxxx.
    The 'demote' operation xxx xxxxxx.
</pre></p>

<p>3. meta_data函数处理</p>

<p>加入状态文件的参数
<pre>
<parameter name="state" unique="1">
<longdesc lang="en">
Location to store the resource state in
</longdesc>
<shortdesc lang="en">State file</shortdesc>
<content type="string" default="${HA_VARRUN}/Stateful-{OCF_RESOURCE_INSTANCE}.state" />
</parameter>
</pre></p>

<p>action模块加入monitor的动作</p>

<p><pre>
<action name="monitor" depth="0" timeout="20" interval="30" role="Master" />
<action name="monitor" depth="0" timeout="20" interval="30" role="Slave" />
</pre>
注意，默认timeout相同时， pacemaker会认为两个timeout是一个操作，需要在crm中进行修改</p>

<p>下面是pacemaker对设置不同interval的说明
<pre>
It is crucial that every monitor operation has a different interval!
This is because Pacemaker currently differentiates between operations only 
by resource and interval;
so if eg. a master/slave resource has the same monitor interval for both roles, 
Pacemaker would ignore the role when checking the status - which would 
cause unexpected return codes,and therefore unnecessary complications.
</pre></p>

<p>4. 添加 demote 和 promote 函数</p>

<p><pre>
    stateful_update() {
        echo $1 > ${OCF_RESKEY_state}
    }</pre></p>

<p>    stateful_check_state() {<br />
        target=$1<br />
        if [ -f ${OCF_RESKEY_state} ]; then<br />
            state=`cat ${OCF_RESKEY_state}`<br />
            if [ &#8220;x$target&#8221; = &#8220;x$state&#8221; ]; then<br />
                return 0<br />
            fi</p>

<p>        else<br />
            if [ &#8220;x$target&#8221; = &#8220;x&#8221; ]; then<br />
                return 0<br />
            fi<br />
        fi</p>

<p>        return 1<br />
    }</p>

<p>    stateful_demote() {<br />
        stateful_check_state<br />
        if [ $? = 0 ]; then<br />
            # CRM Error - Should never happen<br />
            return $OCF_NOT_RUNNING<br />
        fi<br />
        stateful_update &#8220;slave&#8221;<br />
        $CRM_MASTER -v ${slave_score}<br />
        return $OCF_SUCCESS<br />
    }</p>

<p>    stateful_promote() {<br />
        stateful_check_state<br />
        if [ $? = 0 ]; then<br />
            return $OCF_NOT_RUNNING<br />
        fi<br />
        stateful_update &#8220;master&#8221;<br />
        $CRM_MASTER -v ${master_score}<br />
        return $OCF_SUCCESS<br />
    }

对于 master/slave 的agent，必须要有 promote 和 demote 函数</p>

<p>5. start 函数改造</p>

<p><pre>
    返回之前添加
    stateful_update "slave"
    $CRM_MASTER -v ${slave_score}
</pre>
标记自己的状态为slave，顺便告诉pacemaker(使用crm_master来实现)</p>

<p><strong>注意：master/slave资源启动的角色必须是 slave， 然后由pacemaker在slave的节点中选择一个promote为master</strong></p>

<p>6 stop 函数改造</p>

<p><pre>
    $CRM_MASTER -D
    stateful_check_state "master"
    if [ $? = 0 ]; then
        # CRM Error - Should never happen
        return $OCF_RUNNING_MASTER
    fi
    if [ -f ${OCF_RESKEY_state} ]; then
        rm ${OCF_RESKEY_state}
    fi
</pre>
在发送停止命令之前，先进行demote操作</p>

<p>7 monitor函数改造</p>

<p>    将返回的 $OCF_SUCCESS 那一行修改为
<pre>
    stateful_check_state "master"
    if [ $? = 0 ]; then
        if [ $OCF_RESKEY_CRM_meta_interval = 0 ]; then
            # Restore the master setting during probes
            $CRM_MASTER -v ${master_score}
        fi
        return $OCF_RUNNING_MASTER
    fi</pre></p>

<p>    stateful_check_state &#8220;slave&#8221;<br />
    if [ $? = 0 ]; then<br />
        if [ $OCF_RESKEY_CRM_meta_interval = 0 ]; then<br />
            # Restore the master setting during probes<br />
            $CRM_MASTER -v ${slave_score}<br />
        fi<br />
        return $OCF_SUCCESS<br />
    fi</p>

<p>    echo &#8220;File &#8216;${OCF_RESKEY_state}&#8217; exists but contains unexpected contents&#8221;<br />
    return $OCF_ERR_GENERIC

pacemaker充分相信agent，它对资源角色的判断完全来自monitor函数，所有只要我们告诉它自己是 $OCF_RUNNING_MASTER， 它就会认为我们是master</p>

<p>8 加入默认值</p>

<p><pre>
: ${slave_score=5}
: ${master_score=10}</pre></p>

<p>: ${OCF_RESKEY_CRM_meta_interval=0}<br />
: ${OCF_RESKEY_CRM_meta_globally_unique:=&#8221;true&#8221;}</p>

<p>if [ &#8220;x$OCF_RESKEY_state&#8221; = &#8220;x&#8221; ]; then<br />
    if [ ${OCF_RESKEY_CRM_meta_globally_unique} = &#8220;false&#8221; ]; then<br />
        state=&#8221;${HA_VARRUN}/Stateful-${OCF_RESOURCE_INSTANCE}.state&#8221;</p>

<p>        # Strip off the trailing clone marker<br />
        OCF_RESKEY_state=`echo $state | sed s/:[0-9][0-9]*&#46;state/.state/`<br />
    else<br />
        OCF_RESKEY_state=&#8221;${HA_VARRUN}/Stateful-${OCF_RESOURCE_INSTANCE}.state&#8221;<br />
    fi<br />
fi

上面的操作中多次用到了 slave_score 和 master_score 等，现在就对它们进行赋值，要点：只要 master > slave 即可</p>

<p>9 最后，加入动作</p>

<p><pre>
case $1 in</pre></p>

<p>    promote)        stateful_promote<br />
            ;;<br />
    demote)         stateful_demote<br />
            ;;
</p>

<p>好了，agent已经改造完成了，现在展示一下</p>

<p>在pacemaker中的配置如下
<pre>
primitive testklwang ocf:test:klwang \
	op monitor interval="30" role="Master" \
	op monitor interval="29" role="Slave"
</pre></p>

<p>下面是crm_mon的结果：
<pre>
Last updated: Sat Jun  8 12:28:15 2013
Last change: Sat Jun  8 12:28:14 2013 via cibadmin on node1
Stack: cman
Current DC: cent1 - partition with quorum
Version: 1.1.8-7.el6-394e906
5 Nodes configured, 3 expected votes
32 Resources configured.</pre></p>

<p>Online: [ node1 node2 node3 node4 node5 ]</p>

<p> Master/Slave Set: ms_klwang [testklwang]<br />
     Masters: [ node1 ]<br />
     Slaves: [ node2 node3 node4 node5 ]
</p>

<p>现在，我们的目的达到了，已经在集群中选出了一个作为master的节点，下来我们就可以做一些不一样的事情啦</p>

<p><pre>
    stateful_check_state "master"
    if [ $? -eq 0  ]; then
        # 将你想干的事情放在这里
        return $OCF_SUCCESS
    fi
</pre></p>

<p>上面演示中 test:klwang 是使用自带的Dummy示例改造而来，把代码贴出来，也好做个对照
<pre>
#!/bin/sh
#
#
#	Dummy OCF RA. Does nothing but wait a few seconds, can be
#	configured to fail occassionally.
#
# Copyright (c) 2004 SUSE LINUX AG, Lars Marowsky-Brée
#                    All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of version 2 of the GNU General Public License as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it would be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
#
# Further, this software is distributed without any warranty that it is
# free of the rightful claim of any third person regarding infringement
# or the like.  Any license provided herein, whether implied or
# otherwise, applies only to this software file.  Patent licenses, if
# any, provided herein do not apply to combinations of this program with
# other software, or any other product whatsoever.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write the Free Software Foundation,
# Inc., 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
#</pre></p>

<p>#######################################################################<br />
# Initialization:</p>

<p>: ${OCF_FUNCTIONS=${OCF_ROOT}/resource.d/heartbeat/.ocf-shellfuncs}<br />
. ${OCF_FUNCTIONS}<br />
: ${__OCF_ACTION=$1}<br />
CRM_MASTER=&#8221;${HA_SBIN_DIR}/crm_master -l reboot&#8221;</p>

<p>#######################################################################</p>

<p>meta_data() {<br />
	cat <<END
<?xml version="1.0"?>
<!DOCTYPE resource-agent SYSTEM "ra-api-1.dtd">
<resource-agent name="Dummy" version="1.0">
<version>1.0</version></resource-agent></p>

<p><longdesc lang="en">
This is a Dummy Resource Agent. It does absolutely nothing except <br />
keep track of whether its running or not.<br />
Its purpose in life is for testing and to serve as a template for RA writers.</longdesc></p>

<p>NB: Please pay attention to the timeouts specified in the actions<br />
section below. They should be meaningful for the kind of resource<br />
the agent manages. They should be the minimum advised timeouts,<br />
but they shouldn&#8217;t/cannot cover _all_ possible resource<br />
instances. So, try to be neither overly generous nor too stingy,<br />
but moderate. The minimum timeouts should never be below 10 seconds.

<shortdesc lang="en">Example stateless resource agent</shortdesc></p>

<p><parameters>
<parameter name="state" unique="1">
<longdesc lang="en">
Location to store the resource state in.
</longdesc>
<shortdesc lang="en">State file</shortdesc>
<content type="string" default="${HA_VARRUN}/Dummy-{OCF_RESOURCE_INSTANCE}.state" />
</parameter></parameters></p>

<p><parameter name="fake" unique="0">
<longdesc lang="en">
Fake attribute that can be changed to cause a reload
</longdesc>
<shortdesc lang="en">Fake attribute that can be changed to cause a reload</shortdesc>
<content type="string" default="dummy" />
</parameter></p>

<p><parameter name="op_sleep" unique="1">
<longdesc lang="en">
Number of seconds to sleep during operations.  This can be used to test how<br />
the cluster reacts to operation timeouts.
</longdesc>
<shortdesc lang="en">Operation sleep duration in seconds.</shortdesc>
<content type="string" default="0" />
</parameter></p>

<p><parameter name="stateful" unique="1">
<longdesc lang="en">
Location to store the resource stateful in
</longdesc>
<shortdesc lang="en">stateful file</shortdesc>
<content type="string" default="${HA_VARRUN}/Dummy-{OCF_RESOURCE_INSTANCE}.stateful" />
</parameter></p>

<p></p>

<p><actions>
<action name="start" timeout="20" />
<action name="stop" timeout="20" />
<action name="monitor" depth="0" timeout="20" interval="30" role="Master" />
<action name="monitor" depth="0" timeout="20" interval="30" role="Slave" />
<action name="reload" timeout="20" />
<action name="migrate_to" timeout="20" />
<action name="migrate_from" timeout="20" />
<action name="validate-all" timeout="20" />
<action name="meta-data" timeout="5" />
</actions>

END<br />
}</p>

<p>#######################################################################</p>

<p># don&#8217;t exit on TERM, to test that lrmd makes sure that we do exit<br />
trap sigterm_handler TERM<br />
sigterm_handler() {<br />
	ocf_log info &#8220;They use TERM to bring us down. No such luck.&#8221;<br />
	return<br />
}</p>

<p>dummy_usage() {<br />
	cat <<END<br />
usage: $0 {start|stop|monitor|migrate_to|migrate_from|validate-all|meta-data}</p>

<p>Expects to have a fully populated OCF RA-compliant environment set.<br />
END<br />
}</p>

<p>stateful_update() {<br />
	echo $1 > ${OCF_RESKEY_state}<br />
}</p>

<p>stateful_check_state() {<br />
	target=$1<br />
	if [ -f ${OCF_RESKEY_state} ]; then<br />
		state=`cat ${OCF_RESKEY_state}`<br />
		if [ &#8220;x$target&#8221; = &#8220;x$state&#8221; ]; then<br />
		    return 0<br />
		fi</p>

<p>	else<br />
		if [ &#8220;x$target&#8221; = &#8220;x&#8221; ]; then<br />
		    return 0<br />
		fi<br />
	fi</p>

<p>	return 1<br />
}</p>

<p>stateful_demote() {<br />
	stateful_check_state<br />
	if [ $? = 0 ]; then<br />
		# CRM Error - Should never happen<br />
		return $OCF_NOT_RUNNING<br />
	fi<br />
	stateful_update &#8220;slave&#8221;<br />
	$CRM_MASTER -v ${slave_score}<br />
	return $OCF_SUCCESS<br />
}</p>

<p>stateful_promote() {<br />
	stateful_check_state<br />
	if [ $? = 0 ]; then<br />
		return $OCF_NOT_RUNNING<br />
	fi<br />
	stateful_update &#8220;master&#8221;<br />
	$CRM_MASTER -v ${master_score}<br />
	return $OCF_SUCCESS<br />
}</p>

<p>dummy_start() {<br />
    dummy_monitor<br />
    if [ $? =  $OCF_SUCCESS ]; then<br />
		return $OCF_SUCCESS<br />
    fi<br />
    touch ${OCF_RESKEY_state}<br />
	stateful_update &#8220;slave&#8221;<br />
	$CRM_MASTER -v ${slave_score}<br />
}</p>

<p>dummy_stop() {<br />
    dummy_monitor<br />
    if [ $? = $OCF_SUCCESS ]; then<br />
    	$CRM_MASTER -D<br />
    	stateful_check_state &#8220;master&#8221;<br />
    	if [ $? = 0 ]; then<br />
    	    # CRM Error - Should never happen<br />
    	    return $OCF_RUNNING_MASTER<br />
    	fi<br />
    	if [ -f ${OCF_RESKEY_state} ]; then<br />
    	    rm ${OCF_RESKEY_state}<br />
    	fi<br />
		rm ${OCF_RESKEY_state}<br />
    fi<br />
    return $OCF_SUCCESS<br />
}</p>

<p>dummy_monitor() {<br />
	# Monitor _MUST!_ differentiate correctly between running<br />
	# (SUCCESS), failed (ERROR) or _cleanly_ stopped (NOT RUNNING).<br />
	# That is THREE states, not just yes/no.</p>

<p>	sleep ${OCF_RESKEY_op_sleep}<br />
	
	if [ -f ${OCF_RESKEY_state} ]; then<br />
		    stateful_check_state &#8220;master&#8221;<br />
		    if [ $? = 0 ]; then<br />
		        if [ $OCF_RESKEY_CRM_meta_interval = 0 ]; then<br />
		            # Restore the master setting during probes<br />
		            $CRM_MASTER -v ${master_score}<br />
		        fi<br />
		        return $OCF_RUNNING_MASTER<br />
		    fi</p>

<p>		    stateful_check_state &#8220;slave&#8221;<br />
		    if [ $? = 0 ]; then<br />
		        if [ $OCF_RESKEY_CRM_meta_interval = 0 ]; then<br />
		            # Restore the master setting during probes<br />
		            $CRM_MASTER -v ${slave_score}<br />
		        fi<br />
		        return $OCF_SUCCESS<br />
		    fi<br />
		<br />
		    echo &#8220;File &#8216;${OCF_RESKEY_state}&#8217; exists but contains unexpected contents&#8221;<br />
		    return $OCF_ERR_GENERIC<br />
	fi<br />
	if false ; then<br />
		return $OCF_ERR_GENERIC<br />
	fi<br />
	return $OCF_NOT_RUNNING<br />
}</p>

<p>dummy_validate() {<br />
    <br />
    # Is the state directory writable? <br />
    state_dir=`dirname &#8220;$OCF_RESKEY_state&#8221;`<br />
    touch &#8220;$state_dir/$$&#8221;<br />
    if [ $? != 0 ]; then<br />
	return $OCF_ERR_ARGS<br />
    fi<br />
    rm &#8220;$state_dir/$$&#8221;</p>

<p>    return $OCF_SUCCESS<br />
}</p>

<p>: ${slave_score=5}<br />
: ${master_score=10}<br />
: ${OCF_RESKEY_fake=dummy}<br />
: ${OCF_RESKEY_op_sleep=0}<br />
: ${OCF_RESKEY_CRM_meta_interval=0}<br />
: ${OCF_RESKEY_CRM_meta_globally_unique:=&#8221;true&#8221;}</p>

<p>if [ &#8220;x$OCF_RESKEY_state&#8221; = &#8220;x&#8221; ]; then<br />
    if [ ${OCF_RESKEY_CRM_meta_globally_unique} = &#8220;false&#8221; ]; then<br />
	state=&#8221;${HA_VARRUN}/Dummy-${OCF_RESOURCE_INSTANCE}.state&#8221;<br />
	
	# Strip off the trailing clone marker<br />
	OCF_RESKEY_state=`echo $state | sed s/:[0-9][0-9]*&#46;state/.state/`<br />
    else <br />
	OCF_RESKEY_state=&#8221;${HA_VARRUN}/Dummy-${OCF_RESOURCE_INSTANCE}.state&#8221;<br />
    fi<br />
fi</p>

<p>case $__OCF_ACTION in<br />
meta-data)	meta_data<br />
		exit $OCF_SUCCESS<br />
		;;<br />
start)		dummy_start;;<br />
stop)		dummy_stop;;<br />
monitor)	dummy_monitor;;<br />
migrate_to)	ocf_log info &#8220;Migrating ${OCF_RESOURCE_INSTANCE} to ${OCF_RESKEY_CRM_meta_migrate_target}.&#8221;<br />
	        dummy_stop<br />
		;;<br />
migrate_from)	ocf_log info &#8220;Migrating ${OCF_RESOURCE_INSTANCE} to ${OCF_RESKEY_CRM_meta_migrate_source}.&#8221;<br />
	        dummy_start<br />
		;;<br />
promote)	stateful_promote<br />
		;;<br />
demote)		stateful_demote<br />
		;;</p>

<p>reload)		ocf_log err &#8220;Reloading&#8230;&#8221;<br />
	        dummy_start<br />
		;;<br />
validate-all)	dummy_validate;;<br />
usage|help)	dummy_usage<br />
		exit $OCF_SUCCESS<br />
		;;<br />
*)		dummy_usage<br />
		exit $OCF_ERR_UNIMPLEMENTED<br />
		;;<br />
esac<br />
rc=$?<br />
ocf_log debug &#8220;${OCF_RESOURCE_INSTANCE} $__OCF_ACTION : $rc&#8221;<br />
exit $rc
</p>

<p><strong>申明：本文中大量的代码来自<a href="http://clusterlabs.org/" target="_blank">ClusterLabs</a>， 使用时请遵守相关约束</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[让 wordpress 支持手机访问]]></title>
    <link href="http://github.klwang.info/blog/wordpress-enable-mobile-access/"/>
    <updated>2013-06-08T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/wordpress-enable-mobile-access</id>
    <content type="html"><![CDATA[<p>一时兴起，用手机打开了 <a href="http://klwang.info" target="_blank">klwang.info</a>。竟然是无法访问，真是太让人遗憾了。</p>

<p>网上搜了一下，很多人都说使用要 wp-wap， 完了还要改变访问的地址为 <a href="http://klwang.info/?mobile=1" target="_blank">klwang.info/wap</a>， 为了还看，还要另外设置一个域名解析。</p>

<p>我就在想了，既然有了新的访问地址，那可不可以在index.php中加一些代码，判断访问的来源，如果是手机访问，就重定向到 <a href="http://klwang.info/?mobile=1" target="_blank">klwang.info/wap</a> 好了，这样还免得设置解析，最重要的是访问者可以不用输入专门的手机版域名。</p>

<p>接着找了PHP关于$_SERVER的所有变量，挑了一些 可能有用的变量，如</p>

<p><pre>
HTTP_USER_AGENT
VIA
</pre></p>

<p>关于 HTTP_USER_AGENT， 实在是比较强大，可以知道所有的访问来自什么设备，只要grep就行了，具体的设备和 HTTP_USER_AGENT 值的关系，可以参考这篇文档： Mobile Browser ID (User-Agent) Strings， 文章里列出了几乎世界上所有的移动设备HTTP_USER_AGENT信息，为了方便，还提供了下面这么一段代码，来获取这写 AGENT</p>

<p><pre>
// Marc Gray's PHP script (untested by us)
// use at your discretion
<?php
$page = file_get_contents('1.html');
preg_match_all('/<(p) class="g-c-[ns]"[^>]*>(.*?)<\/p>/s', $page, $m); </pre></p>

<p>$agents = array();<br />
foreach($m[2] as $agent) {<br />
  $split = explode(&#8220;\n&#8221;, trim($agent));<br />
  foreach($split as $item) {<br />
    $agents[] = trim($item);<br />
  }<br />
}
// $agents now holds every user agent string, one per array index, trimmed<br />
foreach($agents as $agent) {<br />
 echo($agent.&#8221;\n&#8221;);<br />
}
?>
</p>

<p>代码里边说他们没有测试过，我可以告诉大家，我测试了，确实可以用，但是结果确实很多， 一共 477 行， 下面是其中的几行，写在这里让大家看看 HTTP_USER_AGENT 到底长啥样</p>

<p><pre>
Mobile/9B206 Safari/7534.48.3
Mozilla/5.0 (iPhone; CPU iPhone OS 5_1 like Mac OS X)
AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9B176
Safari/7534.48.3
Mozilla/5.0 (iPhone; CPU iPhone OS 5_0_1 like Mac OS X)
AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A405
Safari/7534.48.3</pre></p>

<p>这里是大量的省略号</p>

<p>Mozilla/5.0 (iPhone; CPU iPhone OS 5_0_1 like Mac OS X)<br />
AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A405<br />
Safari/7534.48.3<br />
Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_3 like Mac OS<br />
X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2
</p>

<p>有了所有的移动设备，就好办了，只要在 HTTP_USER_AGENT 中找这些东东就ok了，只要找到一个，就 Location 到 <a href="http://klwang.info/?mobile=1" target="_blank">klwang.info/wap</a>。<br />
上边的代码改成这样就行了</p>

<p><pre>
// $agents now holds every user agent string, one per array index, trimmed
foreach($agents as $agent) {
 if($agent == $_SERVER['HTTP_USER_AGENT']){
    header("Location: http://klwang.info/wap");
}
</pre></p>

<p>好了，思路有了，也知道怎么搞了，可以动手了， 打开 wordpress，搜索 wap-wp</p>

<p><a href="http://klwang.info/blog/wp-content/uploads/2013/06/wap-wp-search.png"><img src="http://klwang.info/blog/wp-content/uploads/2013/06/wap-wp-search-300x229.png" alt="wap-wp-search" width="300" height="229" /></a></p>

<p>咦？ 这个 Wireless-Wordpress 是啥？ 看了介绍，突然发现我半天的思考白费了，竟然有人已经把我想的事情全做好了，而且还做的更好。</p>

<p>好吧，就直接用人家的插件吧，照着引导设置，也很简单，就不废话了</p>

<p>这下玩大了， 上面说了一大堆，都暴露除了 <a href="http://klwang.info/?mobile=1" target="_blank">klwang.info/wap </a>这个地址，最后竟然不搞了，这怎么能行呢？</p>

<p>嘿嘿，这是难不倒咱们的</p>

<p>在根目录下创建一个 wap/index.php， 里边就一句话</p>

<p><pre>
cat >>wap/index.php<<EOF
<?php
    header("Location: http://klwang.info/?mobile=1");
?>
EOF
</pre></p>

<p>这样，用户就可以正常访问 <a href="http://klwang.info/?mobile=1" target="_blank">klwang.info/wap</a> 啦<br />
总算前面说了那么多没有骗人，也不用收回那么多的废话（打字都打了好久呢）</p>

<p>无图无真相，上两张截图，看看<a href="http://klwang.info/?mobile=1" target="_blank">klwang.info</a>在手机中长得啥样</p>

<p><a href="http://klwang.info/blog/wp-content/uploads/2013/06/wap-klwang-index.png"><img src="http://klwang.info/blog/wp-content/uploads/2013/06/wap-klwang-index.png" alt="wap-klwang-index" width="300" /></a>
主页效果图</p>

<p><a href="http://klwang.info/blog/wp-content/uploads/2013/06/wap-klwang-wordpress-mobile.png"><img src="http://klwang.info/blog/wp-content/uploads/2013/06/wap-klwang-wordpress-mobile.png" alt="wap-klwang-wordpress-mobile" width="300" /></a>
本文效果图</p>

<p>总结：</p>

<p><strong>正式动手前，一定要多观察，说不定别人已经做了你要做的事情了呢</strong>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clusters from Scratch 实验笔记]]></title>
    <link href="http://github.klwang.info/blog/pacemaker-cososync-cman-setup/"/>
    <updated>2013-06-06T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/pacemaker-cososync-cman-setup</id>
    <content type="html"><![CDATA[<p>pacemaker 经典文档 Clusters_from_Scratch 的实验笔记，留作以后参考
<h3>hosts文件设置</h3>
<pre>
# grep node /etc/hosts
192.168.15.11 node1.cluster.com node1
192.168.15.12 node2.cluster.com node2
192.168.15.13 node3.cluster.com node2
</pre></p>

<p>将hosts文件设置成类似上面的样子，并且保证各机器之间网络相通
<h3>配置ssh</h3>
<pre>
# ssh-keygen -t dsa --生成公钥文件，在所有的节点执行
# for i in {1..3}; do ssh node${i} cat /root/.ssh/id_dsa.pub >> /root/.ssh/authorized_keys; done
# for i in {1..3}; do scp /root/.ssh/authorized_keys node${i}:/root/.ssh; done
# for i in {1..3}; do ssh node${i} hostname; done
</pre></p>

<p><h3>设置主机名</h3>
<pre>
# for i in {1..3}; do ssh node${i} sed -i 's/.*//g' /etc/sysconfig/network; done
# for i in {1..3}; do ssh node${i} source /etc/sysconfig/network; done
# for i in {1..3}; do ssh node${i} hostname \$HOSTNAME; done --反斜线很重要，不然所有的机器都长一样啦
</pre></p>

<p>最终的效果：
<pre>
# hostname
node1
# dnsdomainname
cluster.com
</pre></p>

<p><h3>配置软件源</h3>
软件源的配置比较多样化，可以操作俺的另外一篇文章 <a href="http://klwang.info/centos-%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E6%BA%90%EF%BC%88redhat%EF%BC%89/">centos-使用第三方源（redhat）</a>
<h3>安装软件包</h3>
<pre>
# for i in {1..3}; do ssh node${i} yum install -y pacemaker corosync; done
</pre></p>

<p><h3>配置corosync</h3>
在/etc/corosync/corosync.conf文件中，主要调整这些参数
<pre>
mcastaddr: 226.94.1.1 --组播地址
mcastport: 4000 --组播端口(UDP比TCP小一个端口，比如目前设置就使用了tcp:4000和udp:3999)
bindnetaddr: 192.168.16.0 --最后一位是掩码,为（256 - your_mask)，比如我的掩码是255.255.240.0，则第三个点分十进制是16
rrp_mode: passive --文中的集群是3个节点（如果是两个节点的话，就可以保留rrp_mode为none）
</pre></p>

<p>为了让corosync使用pacemaker，需要配置这个文件
<pre>
# cat /etc/corosync/service.d/pcmk
service {
    # Load the Pacemaker Cluster Resource Manager
    name: pacemaker
    ver: 1
}
</pre></p>

<p>完成后，将corosync.conf和pcmk配置文件分发到所有节点上
<h3>启动集群</h3>
<pre>
# service pacemaker start
# crm_mon
Last updated: Thu Aug 27 16:54:55 2009Stack: openais
Current DC: node-1 - partition with quorum
Version: 1.1.5-bdd89e69ba545404d02445be1f3d72e6a203ba2f
3 Nodes configured, 2 expected votes
0 Resources configured.
============
Online: [ node1 node2 node3]
</pre></p>

<p><h3>关闭stonith和忽略quorum</h3>
(虽然三个节点，但是还是希望剩下一个节点的时候可以工作)
<pre>
# crm configure property no-quorum-policy=ignore
# crm configure property stonith-enabled=false
</pre></p>

<p><h3>配置第一个测试资源</h3>
(飘移ip地址)</p>

<p><pre>
# crm configure primitive ClusterIP ocf:heartbeat:IPaddr2 \
> params ip=192.168.15.200 cidr_netmask=32 \
> op monitor interval=30s timeout=60s
# crm_mon
============
Last updated: Fri Aug 28 15:23:48 2009
Stack: openais
Current DC: pcmk-1 - partition WITHOUT quorum
Version: 1.1.5-bdd89e69ba545404d02445be1f3d72e6a203ba2f
3 Nodes configured, 3 expected votes
1 Resources configured.
============</pre></p>

<p>Online: [ node1 node2 node3 ]<br />
ClusterIP (ocf::heartbeat:IPaddr): Started node1
</p>

<p><h3>添加apache服务</h3>
<pre>
# for i in {1..3}; do ssh node${i} yum install -y httpd wget; done
</pre></p>

<p><h3>配置web首页内容</h3>
<pre>
<html>
 <body>My Test Site - node2</body>
</html>
</pre></p>

<p>注意，为了测试ip地址在哪个节点，每个节点的首页文件不能相同</p>

<p>开启apache的状态监控功能
<pre>
<Location /server-status>
   SetHandler server-status
   Order deny,allow
   Deny from all
   Allow from 127.0.0.1

</pre></p>

<p><h3>添加apache的资源</h3>
<pre>
# crm configure primitive WebSite ocf:heartbeat:apache \
> params configfile=/etc/httpd/conf/httpd.conf \
> op monitor interval=1min
</pre></p>

<p><h3>设置colocation约束，将apache和ip地址绑定在一起</h3>
<pre>
# crm configure colocation website-with-ip INFINITY: WebSite ClusterIP
</pre></p>

<p><h3>设置资源的启动和停止顺序</h3>
<pre>
# crm configure order apache-after-ip mandatory: ClusterIP WebSite
</pre></p>

<p><h3>修改资源的prefer节点</h3>
<pre>
# crm configure location prefer-node1 WebSite 100: node1
</pre></p>

<p><h3>增加gfs2支持</h3>
<pre>
# for i in {1..3}; do ssh node${i} yum install -y cman gfs2-utils ccs; done
</pre></p>

<p><h3>修改cman的默认quorum等待时间</h3>
<pre>
# for i in {1..3}; do \
> do ssh node${i} sed -i.sed "s/.*CMAN_QUORUM_TIMEOUT=.*/CMAN_QUORUM_TIMEOUT=0/g" /etc/sysconfig/cman; \
> done
</pre></p>

<p><h3>建立cman的配置文件</h3>
<pre>
# ccs -f /etc/cluster/cluster.conf --createcluster cluster
# for i in {1..3}; do \
> ccs -f /etc/cluster/cluster.conf --addnode node{i}; \
> done
</pre></p>

<p>分发cluster.conf配置文件到各个节点的相应位置<br />
此时，已经不使用corosync.conf配置文件了，其功能由cluster.conf全权代替，关于 cman 和 corosync 之间的不正常关系，请参考我的另外一篇博文 <a href="http://klwang.info/cman-and-corosync/" target="_blank">cman and corosync</a>
<h3>启动集群</h3>
<pre>
service pacemaker start --此时pacemaker会自动启动cman
</pre></p>

<p><h3>建立 gfs2 文件系统资源</h3>
<pre>
# mkfs.gfs2 -p lock_dlm -j 2 -t pcmk:web /dev/drbd1
</pre></p>

<p>关于gfs文件系统的相关问题，可以参考我的另外一篇博文 <a href="http://klwang.info/gfs2-startup/" target="_blank">GFS2 初级</a></p>

<p><h3>在共享文件系统上建立主页测试文件</h3>
<pre>
# mount /dev/drbd1 /mnt/
# cat /mnt/index.html
<html>
 <body>My Test Site - GFS2</body>
</html>
# umount /mnt
</pre></p>

<p><h3>配置文件系统资源</h3>
<pre>
# configure primitive WebFS ocf:heartbeat:Filesystem \
> params device="/dev/drbd/by-res/wwwdata" directory="/var/www/html" fstype="gfs2"
# configure colocation WebSite-with-WebFS inf: WebSite WebFS
# configure order WebSite-after-WebFS inf: WebFS WebSite
</pre></p>

<p><h3>将系统设置为双active的模式</h3>
<pre>
# configure clone WebIP ClusterIP \
> meta globally-unique="true" clone-max="2" clone-node-max="2"
# configure edit ClusterIP
# crm configure clone WebFSClone WebFS
# crm configure clone WebSiteClone WebSite
</pre></p>

<p>至此，整个实验已经搭建完成，整篇博文只是记录了该如何做，没有阐述为什么这么做；关于为什么，可以参考下面的文档，一定会找到比较满意的答案</p>

<p>参考文章
<a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/index.html" target="_blank">Clusters from Scratch</a>
<a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Pacemaker_Explained/index.html" target="_blank">Configuration Explained</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GFS2 初级]]></title>
    <link href="http://github.klwang.info/blog/gfs2-startup/"/>
    <updated>2013-06-05T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/gfs2-startup</id>
    <content type="html"><![CDATA[<p><h4>建立文件系统</h4>
<pre># mkfs.gfs2 -p lock_dlm -t alpha:mydata1 -j 8 /dev/vg01/lvol0</pre>
-p 指定要使用的锁定协议名称，集群的锁定协议为 lock_dlm<br />
-t 这个参数是用来指定集群配置中的 GFS2 文件系统。它有两部分：ClusterName:FSName
<p style="padding-left: 30px;">ClusterName，用来创建 GFS2 文件系统的集群名称<br />
FSName，文件系统名称</p>
-j 指定由 mkfs.gfs2 命令生成的日志数目
<h4>挂载文件系统</h4>
<pre># mount -t gfs2 -o noatime /dev/mapper/mpathap1 /mnt</pre>
使用 noatime 可以避免gfs2在读取文件时更新文件的access时间戳，进而实现读写分离<br />
如果在未启动集群时想单机挂载文件系统，可以使用 lockproto=lock_nolock 参数
<h4>卸载文件系统</h4>
<pre># umount /mnt</pre>
没啥好说的
<h4>配额管理</h4>
挂载的时候使用 quota 选项即可
<pre># mount -o quota=on /dev/vg01/lvol0 /mnt
# quotacheck -ug /mnt                          --创建配额数据库文件
# edquota username                             --设置用户的配额
# quota username                               --验证用户的配额
# edquota -g devel                             --设置组的配额
# quota -g devel                               --验证组的配额</pre>
<h4>容量管理</h4>
<pre># gfs2_grow /mnt</pre>
前提：扩容之前一定要保证gfs2所在的是逻辑卷，并且已经被增大(使用： lvextend)<br />
注意：gfs2只能增大，不能缩小！
<h4>添加日志文件</h4>
gfs2 系统在集群中挂载时，每个节点对应一个日志文件，如果要加入新的节点，就需要添加日志文件
<pre># gfs2_tool journals /mnt                  --察看现在的日志数量
# gfs2_jadd -j1 /mnt                            --添加一个日志文件
</pre>
<h4>挂起gfs2的写入操作</h4>
挂起gfs2的写入操作，可以给管理员提供拷贝文件的机会
<pre>
# dmsetup suspend /mnt                          --挂起写入操作
# dmsetup resume /mnt                           --恢复
</pre></p>

<p><h4>修复文件系统</h4>
<pre>
# fsck.gfs2 -y /mnt
</pre></p>

<p><h4>绑定挂载</h4>
gfs2不支持软链接，可以使用bind多处挂载代替
<pre>
# mount --bind olddir newdir
</pre></p>

<p>参考文档
<a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Global_File_System_2/index.html" title="Global_File_System_2">RedHat_Global_File_System_2</a>
<a href="http://sourceware.org/cluster/doc/usage.txt">gfs2_usage</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[cman and corosync]]></title>
    <link href="http://github.klwang.info/blog/cman-and-corosync/"/>
    <updated>2013-06-04T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/cman-and-corosync</id>
    <content type="html"><![CDATA[<p>在pacemaker的经典文档<a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/index.html" target="_blank">Clusters from Scratch</a> 中，使用到了cman 这个红帽cluster套件中的工具；</p>

<p>奇怪的是，一开始使用的是corosync作为集群的通信层，而到了gfs作为文件系统的active/active时却换成了cman；</p>

<p>同时，corosync.conf中配置的参数貌似也都不起作用了，想要配置两条心跳线也不知道要怎么搞了；</p>

<p>corosync和cman到底是什么关系，到底能不能配置多条心跳线呢？
<h3>cman和corosync的关系</h3>
<p style="padding-left: 30px;">在cman的配置文件cluster.conf中只需要提供集群中节点的机器名就可以了，不需要设置心跳地址；</p>
<p style="padding-left: 30px;">cman会取出$HOSTNAME，然后在cluster.conf中寻找对应的信息（找不到就把dnsdomainname去掉然后再匹配；实在不行，就取出机器的ip地址，然后再去cluster.conf文件中匹配）</p>
<p style="padding-left: 30px;">当前，在寻找心跳ip的过程中，hosts文件起了至关重要的作用</p>
<p style="padding-left: 30px;">cman会自动生成multicast地址，端口号，还有其他的totem参数</p>
<p style="padding-left: 30px;">如果想要知道cman生成corosync参数是什么，可以执行下面的命令</p>
<pre>
    # corosync-objctl -a|grep ^totem
</pre>
<p style="padding-left: 30px;">cman首先找到机器的心跳ip地址，并设置好muliticast信息</p>
<pre>
    totem.interface.ringnumber=0
    totem.interface.bindnetaddr=192.168.1.29
    totem.interface.mcastaddr=239.192.209.5
    totem.interface.mcastport=5405
</pre>
<p style="padding-left: 30px;">然后，cman会将下面的这些参数设置的比一般的corosync集群稍微高一些</p>
<pre>
    totem.token=10000
    totem.join=60
    totem.fail_recv_const=2500
    totem.consensus=12000
</pre>
<p style="padding-left: 30px;">下来，cman开启通信加密（根据节点的数量，设置rrp_mode的值）</p>
<pre>
    totem.rrp_mode=none
    totem.secauth=1
    totem.key=composers
</pre>
<p style="padding-left: 30px;">最后，cman将集群的quorum提供者设置成它自己</p>
<pre>
    quorum.provider=quorum_cman
</pre></p>

<p><h3>设置多条心跳线</h3>
<p style="padding-left: 30px;">cman的第一条心跳线是根据hosts文件取出的（当然，也可以在clusternode的name中直接写ip地址）</p>
<p style="padding-left: 30px;">比如：</p>
<pre>
    <clusternode name="LIN01-adm" nodeid="1" votes="1" />
</pre>
<p style="padding-left: 30px;">cman会从host文件中取LIN01-adm对应的ip地址作为心跳地址；</p>
<p style="padding-left: 30px;">需要添加多条心跳线时，就要增加altername选项了</p>
<p style="padding-left: 30px;">比如：</p></p>

<p><pre>
    <clusternode name="cent1" nodeid="1">
      <altname name="192.148.1.1" />
    </clusternode>
</pre>
<p style="padding-left: 30px;">这样，就多了一条192.168.1.1的心跳线了(当然也可以写名字，然后cman自动从hosts文件中解析地址)</p></p>

<p><h3>相关工具的使用</h3>
<p style="padding-left: 30px;">既然使用了cman，就索性把红帽的其他工具也好好用一下<br />
ccs-红帽的集群配置工具（专门用来写cluster.conf文件的）</p></p>

<p><p style="padding-left: 30px;">建立集群</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --createcluster cluster_name
</pre>
<p style="padding-left: 30px;">添加节点</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --addnode nodename
</pre>
<p style="padding-left: 30px;">删除节点</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --rmnode nodename
</pre>
<p style="padding-left: 30px;">添加备用名（备用心跳）</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --addalt nodename alt_name
</pre>
<p style="padding-left: 30px;">删除备用名</p>
<pre>
    ccs -f /etc/cluster/cluster.conf --rmalt nodename alt_name
</pre>
<p style="padding-left: 30px;">其他命令 man ccs 即可</p>
    cman_tool
<p style="padding-left: 30px;">写好了 custer.conf 文件，我们就需要在集群中发布一下，也很简单</p>
<p style="padding-left: 30px;">前提条件，设置好 ricci 用户的密码，开启 ricci 服务</p>
<pre>
    service ricci start &amp;&amp; chkconfig ricci on
    password ricci
</pre>
<p style="padding-left: 30px;">分发配置文件</p>
<pre>
    cman_tool versiom -r
</pre></p>

<p><p style="padding-left: 30px;">其他工具，大家自己探索吧</p></p>

<p>参考文档
<a href="http://chrissie.fedorapeople.org/CmanYinYang.pdf" title="CmanYinYang">CmanYinYang</a>
<a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html-single/Cluster_Administration/index.html" title="RedHat_Cluster_Administration" target="_blank">RedHat_Cluster_Administration</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kvm 虚拟机 fence 设置]]></title>
    <link href="http://github.klwang.info/blog/kvm-guest-fence/"/>
    <updated>2013-06-03T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/kvm-guest-fence</id>
    <content type="html"><![CDATA[<p>习惯于使用 虚拟机 测试HA配置文件，但是对于fence这块，一直没有办法搞定</p>

<p>之前看到有人用external/ssh的，就想试试，但是后知后觉的发现，这玩意竟然被pacemaker给枪毙了（<a href="http://hg.linux-ha.org/glue/rev/5ef3f9370458">相关链接</a>）官方说来说去就是一句话来解释这个问题(No. external/ssh simply cannot be relied on)
<pre>
>> You really don't want to rely on SSH STONITH in a production environment.
>>
>> Regards,
>>
>> Tim
>>
>> Sure, but I'm in a lab environment at the moment without
>> UPS-based STONITH capabilities, so having SSH STONITH working
>> to test things out would be helpful.</pre></p>

<p>The development package should contain external/ssh.</p>

<p>> Also, even in production,<br />
> what would be the harm of having both SSH and UPS-based STONITH<br />
> available? Wouldn&#8217;t more routes to STONITH a node be better?</p>

<p>No. external/ssh simply cannot be relied on.
算了，不给用就不用好了，kvm 反正还有其他的方式来实现fence呢
<h3>host机操作</h3>
<h4>1 把包先装上</h4>
<pre># yum install -y fence-virt fence-virtd fence-virtd-libvirt fence-virtd-multicast</pre>
<h4>2 搞一个认证文件</h4>
没有啥限制，主要是内部通信用，通信机器之间文件一致即可
<pre># mkdir /etc/cluster
# dd if=/dev/urandom of=/etc/cluster/fence_xvm.key bs=4096 count=1</pre>
<h4>3. 生成配置文件</h4>
<pre># fence_virtd -c</pre>
有一点需要改改，其他的按照提示一路回车就ok
<pre># Backend module [checkpoint]: libvirt  --因为我们只安装了fence-virtd-libvirt这个backend</pre>
<h4>4.配置文件review</h4>
生成的配置文件长这个样 /etc/fence_virt.conf
<pre>backends {
    libvirt {
        uri = "qemu:///system";
    }</pre></p>

<p>}</p>

<p>listeners {<br />
    multicast {<br />
        interface = &#8220;br0&#8221;;<br />
        port = &#8220;1229&#8221;;<br />
        family = &#8220;ipv4&#8221;;<br />
        address = &#8220;225.0.0.12&#8221;;<br />
        key_file = &#8220;/etc/cluster/fence_xvm.key&#8221;;<br />
    }</p>

<p>}</p>

<p>fence_virtd {<br />
    module_path = &#8220;/usr/lib64/fence-virt&#8221;;<br />
    backend = &#8220;libvirt&#8221;;<br />
    listener = &#8220;multicast&#8221;;<br />
}
就是一般的配置文件，对于不明白的地方，man fence_virt.conf 即可找到喜欢的解释
<strong><em>特别注意： interface = &#8220;br0&#8221;; 这个参数一定要仔细对待，不要被它的默认提示给忽悠了，这个监听的网卡一定要选择一个host机和guest机能够相互通信的网卡（像virtbr0这种kvm默认的网卡要仔细喽），比如，如果你的guest机是bridge的，那就写host的对外网卡就ok了</em></strong>
<h4><strong><em></em></strong> 5.启动服务</h4>
<pre># service fence_virtd start
# chkconfig --add fence_virtd
# chkconfig fence_virtd on</pre>
<h4>6. 验证配置</h4>
<pre># fence_xvm -o list</pre></p>

<p>cluster_node1             f2bd9d70-411e-393c-0720-3311985a63bf on<br />
cluster_node2             3fd4a9cd-aa43-11b1-a943-9f8623b790b3 on<br />
cluster_node3             f2bd9d70-411e-393c-0720-3314335a34bf on<br />
cluster_node4             3fd4a9cd-aa43-11b1-a943-9f234b7934b3 on
重启一个guest机试试？（不要说我没告诉你-o reboot是重启的意思哦）
<pre># fence_xvm -o reboot -H cluster_node2</pre>
<h3>guest机配置</h3>
<h4>1. 安装软件包</h4>
这个 fence-virt 就是为了试试 fence_xvm 命令的，其实不装也ok的
<pre># yum install -y fence-virt</pre>
<h4>2. 同步配置文件</h4>
把之前在host上生成的 /etc/cluster/fence_xvm.key 文件拷贝到所有guest的对应位置
<h4>3. 测试</h4>
<pre># fence_xvm -o list</pre></p>

<p>cluster_node1             f2bd9d70-411e-393c-0720-3311985a63bf on<br />
cluster_node2             3fd4a9cd-aa43-11b1-a943-9f8623b790b3 on<br />
cluster_node3             f2bd9d70-411e-393c-0720-3314335a34bf on<br />
cluster_node4             3fd4a9cd-aa43-11b1-a943-9f234b7934b3 on
同样，可以看到其它的机器，<br />
试试？别把自己fence掉啦
<pre># fence_xvm -o reboot -H cluster_node2</pre>
ok啦，可以使用这个fence设备了
<pre> crm configure primitive st-virt stonith:fence_xvm \
 params port="cluster_node1 cluster_node2 cluster_node3 cluster_node4"</pre>
这个port就是fence机器的列表</p>

<p>&nbsp;</p>

<p>参考文章
<a href="http://www.daemonzone.net/e/3/" target="_blank">Guest fencing on a RHEL KVM host</a>
<a href="http://clusterlabs.org/wiki/Guest_Fencing" target="_blank">Guest Fencing</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux 文件权限]]></title>
    <link href="http://github.klwang.info/blog/linux-file-permission/"/>
    <updated>2013-06-02T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/linux-file-permission</id>
    <content type="html"><![CDATA[<p>之前遇到一个文件权限是 rws&#8211;x&#8211;x 的问题，对这个 -s 属性很是不太明白，就仔细的找了些资料；<br />
赶紧记录下来，以免后面又忘了</p>

<p>1. 基本权限</p>

<p>权限位一共是10位，下面这个图表比较清楚的解释了每一位的作用
<pre>1	2	3	4	5	6	7	8	9	10
File	User Permissions	Group Permissions	Other Permissions
Type	Read	Write	Execute	Read	Write	Execute	Read	Write	Execute
d	r	w	e	r	w	e	r	w	e</pre>
<ul>
	<li>1位是type: - 常规文件, d 目录, l 链接.</li>
</ul>
<ul>
	<li>2-4位表示文件拥有者的权限:分别是读、写、和执行</li>
</ul>
<ul>
	<li>5-7位表示同组用户的权限：分别是读、写、和执行</li>
</ul>
<ul>
	<li>8-10位表示其他用户的权限：同样，分别是读、写、和执行</li>
</ul>
文件的权限一个共有5种，分别是
<ul>
	<li>r = 读 - r权限只在read位置出现</li>
</ul>
<ul>
	<li>w = 写 - w权限只在write位置出现</li>
</ul>
<ul>
	<li>x = 执行 - w权限只在execute位置出现</li>
</ul>
<ul>
	<li>s = setuid - s权限只在execute位置出现</li>
</ul>
<ul>
	<li>- 表示没有权限</li>
</ul>
type位一共有这么几种
<ul>
	<li>d = 目录</li>
</ul>
<ul>
	<li>l = 符号链接</li>
</ul>
<ul>
	<li>s = socket</li>
</ul>
<ul>
	<li>p = 命名管道</li>
</ul>
<ul>
	<li>- = 普通文件</li>
</ul>
<ul>
	<li>c= character (没有缓冲的) 设备，如键盘，鼠标</li>
</ul>
<ul>
	<li>b= block (有缓冲的) 设备，如IDE硬盘</li>
</ul>
文件的 Set ID 位
<ul>
	<li>当owner的execute被设置为s时，表示任何程序在执行这个文件时就好像文件的owner一样（可以接触到相关的系统资源和权限等）</li>
</ul>
<ul>
	<li>同理，当group的execute被设置为s时，表示任何程序在执行这个文件时就好像文件的group中成员一样可以接触到相关的系统资源和权限</li>
</ul>
<ul>
	<li>最后，other肯定是没有s位的，不然就太恐怖啦（你能使用other的权限，那还得了？）</li>
</ul>
<ul>
	<li><em>import：设置s时，首先要保证在该位上有x权限（常识啦，没有x就没有意义么）</em></li>
</ul>
<em> </em>
目录有两个特殊的权限位，分别是
<ul>
	<li>t 表示在这个目录中，用户只能删除或者修改属于自己的或者自己有写权限的文件，比如/tmp就有这个权限</li>
</ul>
<ul>
	<li>s 设置group id，一般来说，一个用户建立一个文件时，文件的组一般为自己所属的组。。当目录设置了该位时，该目录下建立的文件组和目录所属的组相同，而不是建立文件的用户所属组</li>
</ul>
umask设置
<p style="padding-left: 30px;">
文件默认的权限为<br />
777 可执行文件<br />
666 普通文件</p>
文件默认的权限会让所有用户对可执行文件有所有权限，对所有普通文件有读写权限，设置umask可以改变这个系统行为</p>

<p>如，umask为022时，普通文件的权限就变成了 666 - 022 = 644， 可执行文件权限变成了 777 - 022 = 755</p>

<p>over</p>

<p>参考文档
<a title="Linux Files and File Permission" href="http://www.comptechdoc.org/os/linux/usersguide/linux_ugfilesp.html" target="_blank">Linux Files and File Permission</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[设置网卡名]]></title>
    <link href="http://github.klwang.info/blog/config-your-net-card-name/"/>
    <updated>2013-06-01T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/config-your-net-card-name</id>
    <content type="html"><![CDATA[<p>有时候更换网卡之后，发现网卡名字会变得很奇怪，比如，明明自己只有两块网卡，却叫 eth2，eth3；</p>

<p>那eth和eth1跑哪里去了？<br />
这个是/etc/udev/rules.d/70-persistent-net.rules文件的内容
<pre>
# This file was automatically generated by the /lib/udev/write_net_rules
# program, run by the persistent-net-generator.rules rules file.
#
# You can modify it, as long as you keep each rule on a single
# line, and change only the value of the NAME= key.
</pre>
人家说的很清楚，这个文件是由 /lib/udev/write_net_rules 自动生成的，但是还是可以根据需要手工修改，下面是这个文件本来的内容</p>

<p><pre>
# PCI device 0x10ec:/sys/devices/pci0000:00/0000:00:1c.2/0000:03:00.0 (r8169)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="00:e0:4c:ca:xa:ba", ATTR{dev_id}=="0x0", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"</pre></p>

<p># PCI device 0x168c:/sys/devices/pci0000:00/0000:00:1c.1/0000:02:00.0 (ath9k)<br />
SUBSYSTEM==&#8221;net&#8221;, ACTION==&#8221;add&#8221;, DRIVERS==&#8221;?*&#8221;, ATTR{address}==&#8221;48:5d:60:ca:xa:a9&#8221;, ATTR{dev_id}==&#8221;0x0&#8221;, ATTR{type}==&#8221;1&#8221;, KERNEL==&#8221;wlan*&#8221;, NAME=&#8221;eth1&#8221;</p>

<p># PCI device 0x10ec:/sys/devices/pci0000:00/0000:00:1c.2/0000:03:00.0 (r8169)<br />
SUBSYSTEM==&#8221;net&#8221;, ACTION==&#8221;add&#8221;, DRIVERS==&#8221;?*&#8221;, ATTR{address}==&#8221;00:ca:xa::74:90:ba&#8221;, ATTR{dev_id}==&#8221;0x0&#8221;, ATTR{type}==&#8221;1&#8221;, KERNEL==&#8221;eth*&#8221;, NAME=&#8221;eth2&#8221;</p>

<p># PCI device 0x168c:/sys/devices/pci0000:00/0000:00:1c.1/0000:02:00.0 (ath9k)<br />
SUBSYSTEM==&#8221;net&#8221;, ACTION==&#8221;add&#8221;, DRIVERS==&#8221;?*&#8221;, ATTR{address}==&#8221;ca:xa::60:95:07:c9&#8221;, ATTR{dev_id}==&#8221;0x0&#8221;, ATTR{type}==&#8221;1&#8221;, KERNEL==&#8221;wlan*&#8221;, NAME=&#8221;eth3&#8221;
</p>

<p>哦，原来是这样的，之前的两块网卡已经换了，信息却还是留在了这里；<br />
那好吧，删掉那两行内容，把 eth2 、 eth3分别改成 eth0和eth1即可</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ovirt 安装小记]]></title>
    <link href="http://github.klwang.info/blog/ovirt-install-guide-of-klwang/"/>
    <updated>2013-05-28T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/ovirt-install-guide-of-klwang</id>
    <content type="html"><![CDATA[<p>出于工作需要，试了一把Ovirt, 发现其和cloudstack、openstack这种大家伙还是有差距的，不过也有它的长处，今天就记录一下安装的过程，下次也就方便一些了。。。
<h3>写在前面：</h3>
硬件要求： CPU支持虚拟化肯定是必须的了<br />
作为存储的节点，硬盘相应的大一些喽<br />
运行虚拟机的节点，RAM和CPU肯定是越犀利越<br />
数据库和管理节点，差不多就ok</p>

<p>操作系统： 本次用的是 CentOS 6.4 x86_64， 反正红帽系列都ok，debian系列的没有试过，但是应该也差不多</p>

<p>文档来源： Ovirt的好处就是文档多（当然，都是红帽的RHEV，工具的名字可能不一样，功能、参数差不多）
<h3>进入正题：</h3>
<h4>1. 防火墙设置</h4>
<h5>管理节点</h5>
<pre>    8080、8443： --web管理界面
    8006~8009： --和host通信</pre>
<h5>存储节点(本次使用nfs)</h5>
<pre>    $ cat /etc/sysconfig/nfs
    LOCKD_TCPPORT=32803
    LOCKD_UDPPORT=32769
    MOUNTD_PORT=892
    RQUOTAD_PORT=875
    STATD_PORT=662
    STATD_OUTGOING_PORT=2020</pre>
<h5>host节点</h5>
<pre>    5634 - 6166： 虚拟机通信(kvm等)
    16514： libvirt在线迁移
    49152 - 49216：虚拟机迁移、fencing等
    54321： host和manager通信（坑爹的端口哦）</pre>
<h5>Directory节点</h5>
<pre>    88, 464: Kerberos
    389, 636: ldap</pre>
最后，ssh的22端口肯定要开了，我们默认所有节点都安装ssh服务。。</p>

<p>要是单单自己装着玩，为了省事，就直接这样吧
<pre>    service iptables stop
    chkconfig --del iptables</pre>
<h4>2. 准备工作</h4>
Ovrit比较坑爹，需要一个DNS服务器来给自己干活，hosts文件都抗不住，最好装一个;<br />
用自己的机子，或者随便一台host或者管理机兼职一下</p>

<p>为每台机子设置好机器名、域名等: 达到以下结果(hosts文件就能办到)
<pre>    hostname --fqdn
        nodex.xx.com
    dnsdomainname
        xx.com</pre>
<h4>3. 安装管理节点</h4>
<pre>    cd /etc/yum.repo.d/
    wget http://ovirt.org/releases/stable/ovirt-engine.repo
    yum install ovirt-engine</pre>
ok,就是这么简单</p>

<p>配置：
<pre>    # engine-setup</pre>
会问一大堆东西，一般默认就ok，记住一点，数据库和admin的密码一定要记住，不然就自己后悔去吧
<pre>    oVirt Manager will be installed using the following configuration:
    =================================================================
    http-port: 8080
    https-port: 8443
    host-fqdn: engine.demo.ovirt.org
    auth-pass: ********
    db-pass: ********
    org-name: oVirt
    default-dc-type: NFS
    nfs-mp: /isoshare
    iso-domain-name: ISODomain
    override-iptables: yes
    Proceed with the configuration listed above? (yes|no):</pre>
数据库密码忘了还好，用的是postgresql，可以参考我的<a href="http://klwang.info/pgsql%E5%BF%98%E8%AE%B0%E5%AF%86%E7%A0%81%E5%A4%84%E7%90%86%E5%8A%9E%E6%B3%95/">这篇blog</a>搞定<br />
admin的密码忘了我就不知道怎么搞了，自己重新 engine-setup 吧</p>

<p>装完后，会自动把你的网卡桥接，做一个ovirtmanage的网卡，供它自己使用<br />
这里，如果你已经把iptables禁用了，就不要让它帮你设置iptables了
<h4>4. 设置</h4>
管理节点安装好，就可以进去操作一把了，地址 your_manager_ip:8080<br />
帐号密码是 admin/你的密码</p>

<p>概念和 cloudstack 差不多，有 datacenter、cluster、host、storage的概念，基本是相互包含的关系<br />
没有pods的概念，把 secondary 分为了 iso 和 export连个玩意，意思差不多，放iso文件和备份啥的</p>

<p>特别的，storage支持iscsi，大家可以试试</p>

<p>最终安装结果：<br />
一个default的datacenter、一个default的cluster、加一个放data的storage（我用的是NFS，NFS的安装设置也不是很难，参考我之前<a href="http://klwang.info/category/cloudstack/">转载的那个cludstack</a>里边的做法即可）<br />
然后，把 data的storage激活(active)，这个云就算是ok了，，，下来可以添加运行虚拟机的节点啦
<h4>5. 安装host节点</h4>
红帽就是红帽，不愧是做操作系统的；<br />
他们做了一个专门运行host的操作系统，也就是说让他们的host直接运行在裸机上<br />
我们就算了，在centos上装个运行虚拟机的服务就行了，不折腾了<br />
想折腾的朋友，看看这里：http://resources.ovirt.org/releases/stable/iso， 随便下载个试试</p>

<p>ok，装服务
<pre>    yum install vdsm vdsm-cli -y</pre>
就这么简单，同样，它会自己搞定网卡桥接的事情
<h4>6. 将host节点加入云</h4>
在管理界面中，点击hosts栏，点击add<br />
输入 显示名， 地址， root的 密码就ok了，会自动安装需要的软件，完后自动重启</p>

<p>最后，选中一个host机器，点击 active 激活就行了
<h4>7. 创建虚拟机</h4>
先把 iso 文件传上去， scp，嘿嘿。放在 engine-setup 时你自己设定的： nfs-mp 目录下（不会忘了吧）<br />
刷新一下，知道你看见自己上传的系统镜像iso</p>

<p>创建虚拟机很简单，自己设置一下网卡，默认的ovirt-manage网卡会桥接到自己的host机器上去<br />
完了，根据需要添加一个磁盘<br />
好了，剩下的就是平时装系统，不浪费篇幅说了
<h4>8. 制作模版</h4>
这个比较重要，云就是为了部署起来简单，这样的话，模版就不可或缺了</p>

<p>选一个要作为模版的虚拟机，开始处理：<br />
Linux
<pre>    touch /.unconfigured
    rm -rf /etc/ssh/ssh_host_*
    rm -rf /etc/udev/rules.d/70-*
    去除 /etc/sysconfig/network-scripts/ifcfg-eth* 中HWADDR那行
    删了 /var/log 下的垃圾（可选）
    init 0</pre>
好了，可以做模版了</p>

<p>Windows<br />
运行 Sysprep, 做好模版即可（一般就是一路回车搞定）</p>

<p>ok，有了模版就可以使用模版生成虚拟机了
<h4>9. 在线迁移</h4>
云的最大好处就是可以将虚拟机在线迁移到其他节点，操作很简单，图形界面，点击 migrage 就行啦 自己试试吧，只可意会不能言传的，这东西
<h4>10. 用户管理</h4>
Ovirt的用户管理真难受，自己还不管，非要搞个认证服务器来，我们使用IPA<br />
IPA安装（不能和manage在一个节点哦）：
<pre>    yum install ipa-server bind bind-dyndb-ldap
    ipa-server-install</pre>
一路按照提示输入信息<br />
切记切记要设置好DNS服务器，安装过程后会自动识别机器名和域名<br />
最后，这句话：
<pre>    Sample zone file for bind has been created in /tmp/sample.zone.ygzij5.db</pre>
生成了一个关于ldap、krb等的DNS解析，如下：
<pre>    ; ldap servers
    _ldap._tcp              IN SRV 0 100 389        ipaserver.example.com
    ;kerberos realm
    _kerberos               IN TXT EXAMPLE.COM
    ; kerberos servers
    _kerberos._tcp          IN SRV 0 100 88         ipaserver.example.com
    _kerberos._udp          IN SRV 0 100 88         ipaserver.example.com
    _kerberos-master._tcp   IN SRV 0 100 88         ipaserver.example.com
    _kerberos-master._udp   IN SRV 0 100 88         ipaserver.example.com
    _kpasswd._tcp           IN SRV 0 100 464        ipaserver.example.com
    _kpasswd._udp           IN SRV 0 100 464        ipaserver.example.com</pre>
将这些东西放到你的DNS服务器的 解析文件中去
<pre>    service named reload</pre>
添加用户
<pre>    kinit admin
    ipa user-add
    ipa password login-name --循环添加多个用户
    kdestory</pre>
然后，在你的Ovirt中Uers中设置用户权限，就可以用新添加的用户登陆啦。。
<h4>后记</h4>
文章写的比较简单，默认读者应该 对linux比较熟悉</p>

<p>参考文章:</p>

<p><a href="https://access.redhat.com/site/documentation//en-US/Red_Hat_Enterprise_Linux/6/html/Identity_Management_Guide/index.html">Red_Hat_Identity_Management_Guide</a>
<a href="http://www.ovirt.org/File:OVirt-3.0-Installation_Guide-en-US.pdf">OVirt-3.0-Installation_Guide</a>
<a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Virtualization/3.2-Beta/html/Administration_Guide/index.html" target="_blank">RHEV_Administration_Guide</a>
<a href="https://access.redhat.com/site/documentation//en-US/Red_Hat_Enterprise_Virtualization/3.2-Beta/html/Installation_Guide/index.html">RHEV_Installation_Guide</a>
<a href="https://access.redhat.com/site/documentation//en-US/Red_Hat_Enterprise_Virtualization/3.2-Beta/html/Quick_Start_Guide/index.html">RHEV_Quick_Start_Guide</a>
<a href="http://www.rjsystems.nl/en/2100-dns-discovery-openldap.php">DNS discovery for OpenLDAP</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[resource agent 的调试方式]]></title>
    <link href="http://github.klwang.info/blog/resource-agent-debug/"/>
    <updated>2013-05-27T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/resource-agent-debug</id>
    <content type="html"><![CDATA[<p>HA的配置过程中，经常会出现资源不按照自己想像的方式启动的时候，又苦于不知道该怎么寻找问题；</p>

<p>自己平时也遇到过一些，总结了一些经验，今天突然发现clusterlibs的wiki里边用的方式和我用的方式基本一致，就做一下整理，希望可以帮助到别人；</p>

<p>1. 脱管资源
<pre>    crm resource unmanage <rsc_name>;</rsc_name></pre>
2. 配置环境变量
<pre>    # export OCF_ROOT=/usr/lib/ocf
    --这个是必须有的，默认是这个位置，可根据自己的环境设置，为resource,d的父目录即可
    # export OCF_RESKEY_=  
    --这些都是配置时需要传进去的参数
    # 如果传进去的参数比较多，这里一一设置，注意 OCF_RESKEY 的前缀</pre>
3. 手工启动试试;
<pre>    # /usr/lib/ocf/resource.d/heartbeat/<rsc_name> start ; echo $?</rsc_name></pre>
瞅瞅返回的参数，要是还是没有有用的信息，就像平时调试shell一样，加一个 -x 参数</p>

<p>4. 调试
<pre>    # sh -x /usr/lib/ocf/resource.d/heartbeat/<rsc_name>; start ; echo $?</rsc_name></pre>
这样，基本可以找到问题所在了，修复问题（修改agent脚本，或者配置忘记的参数，或者其他）</p>

<p>5. 重新让paceker管理资源
<pre>    crm resource manage <rsc_name /></pre>
<a href="http://clusterlabs.org/wiki/Debugging_Resource_Failures">原文地址</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudstack 中 reset_password 功能设置]]></title>
    <link href="http://github.klwang.info/blog/use-of-cloudstack-reset_password-method/"/>
    <updated>2013-05-26T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/use-of-cloudstack-reset_password-method</id>
    <content type="html"><![CDATA[<p>Cloudstack中对于虚拟机，有一个reset_password的功能，一直疑惑这玩意是干啥的</p>

<p>没错，就是重置root或者Administrator密码的，下面写下配置的过程</p>

<p>Windows</p>

<p>windows的比较简单，按照官方文档、安装一个小程序就ok了</p>

<p>点击<a href="http://sourceforge.net/projects/cloudstack/files/Password%20Management%20Scripts/CloudInstanceManager.msi">这里</a>下载</p>

<p>一路回车安装即可</p>

<p>Linux</p>

<p>点击<a href="http://sourceforge.net/projects/cloudstack/files/Password%20Management%20Scripts/cloud-set-guest-password">这里</a>下载
<pre>    mv cloud-set-guest-password /etc/rc.d/init.d
    chmod +x /etc/init.d/cloud-set-guest-password
    chkconfig --add cloud-set-guest-password</pre>
很明显，上面说的 redhat 系列的操作系统，在ubuntu上需要修改一些东西</p>

<p>比如，脚本的这行
<pre>    DHCP_FILES="/var/lib/dhclient/dhclient-eth0.leases /var/lib/dhcp3/dhclient.eth0.leases"</pre>
先看看操作系统的dhcp文件是不是叫那个名字，要是不是的话，就把最新的复制一个，取名dhclient.eth0.leases
<pre>    mv cloud-set-guest-password /etc/init.d
    chmod +x /etc/init.d/cloud-set-guest-password
    update-rc.d -n cloud-set-guest-password defaults</pre>
ok，可以测试一下，脚本本身比较简单，要是不工作的话，自己修改修改，调试调试，多试几次</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CentOS 使用第三方源（redhat）]]></title>
    <link href="http://github.klwang.info/blog/centos-and-centos-update-softwares/"/>
    <updated>2013-05-26T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/centos-and-centos-update-softwares</id>
    <content type="html"><![CDATA[<p>干活使用的机器装的是redhat，但是又木有买人家的服务，要安装软件实在是不容易，整理一些有用的源，作为笔记，免得以后忘了；
<h3>epel源</h3>
1. 为了保证源的顺序，安装 yum-priorities
<pre>    yum install -y yum-priorities</pre>
2. 安装 epel.repo，在这里找，根据自己的版本，选择i386或者x86_86(这里选择x86_64的)
<pre>    wget http://mirrors.ustc.edu.cn/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm 
    rpm -ivh epel-release-6-8.noarch.rpm
    rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6</pre>
3. 设置priority，比其他的源数字都大</p>

<p>ok， epel的源安装好啦
<h3>rpmforge 源</h3>
1. 在这里下载对应的版本 http://pkgs.repoforge.org/rpmforge-release/
<pre>    wget http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm
    rpm -ivh rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm</pre>
2. 不导入证书的话，将gpgcheck设置为0即可</p>

<p>3. 设置priority，比其他的源数字都大</p>

<p>ok, rpmforge源安装好了
<h3>rpmfusion 源</h3>
1. 在这里下载对应的版本 http://download1.rpmfusion.org/nonfree/el/updates/6/
<pre>    wget http://download1.rpmfusion.org/nonfree/el/updates/6/x86_64/rpmfusion-nonfree-release-6-1.noarch.rpm
    rpm -ivh rpmfusion-nonfree-release-6-1.noarch.rpm</pre>
2. 设置priority，比其他的源数字都大
<h3>redhat使用centos源</h3>
1. 删除系统自带的yum相关软件：rpm -qa | grep yum | xargs rpm -e &#8211;nodeps<br />
2. 找一个镜像站点，下载对应的包(俺们选择163的)
<pre>    python-iniparse
    yum
    yum-fastestmirror
    yum-metadata-parser</pre>
3. 安装下载好的包
<pre>    rpm -ivh *.rpm</pre>
4. 下载163的源
<pre>    cd /etc/yum.repo.d
    wget mirrors.163.com/.help/CentOS6-Base-163.repo</pre>
5. 导入证书
<pre>    rpm --import http://tel.mirrors.163.com/centos/6.4/os/x86_64/RPM-GPG-KEY-CentOS-6</pre>
ok， 可以使用centos的源啦</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Split-brain, Quorum, and Fencing]]></title>
    <link href="http://github.klwang.info/blog/split-brain-quorum-and-fencing/"/>
    <updated>2013-05-19T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/split-brain-quorum-and-fencing</id>
    <content type="html"><![CDATA[<p>原文地址：  <a href="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html">Split-brain, Quorum, and Fencing</a>,一切权利归原作者所有</p>

<p>in some ways, an HA system is pretty simple - it starts services, it stops them, and it sees if they and the computers that run them are still running.  But, there are a few bits of important &#8220;rocket science&#8221; hiding in there among all these apparently simple tasks.  Much of the rocket science that&#8217;s there centers around trying to solve a single thorny problem - split brain.  The methods that are used to solve this problem are quorum and fencing.  Unfortunately, if you manage an HA system you need to understand these issues.  So this post will concentrate on these three topics: <em>split-brain, quorum, </em>and <em>fencing</em>.</p>

<p>If you have three computers and some way for them to communicate with each other, you can make a cluster out of them and,each can monitor the others to see if their peer has crashed.  Unfortunately, there&#8217;s a problem here - you can&#8217;t distinguish a crash of a peer from broken communications with the peer.  All you really know is that you can&#8217;t hear anything from them.  You&#8217;re really stuck in a Dunn&#8217;s law[<a href="http://linux-ha.org/DunnsLaw">1</a>] situation - where you really don&#8217;t know very much, but desperately need to.  Maybe you don&#8217;t feel too desperate yet.  Perhaps you think that you don&#8217;t need to be able to distinguish these two cases.  The truth is that sometimes you don&#8217;t need to, but much of the time you very much need to be able to tell the difference.  Let&#8217;s see if I can make this clearer with an illustration.</p>

<p>Let&#8217;s say you have three computers, <em>paul</em>, <em>silas</em>, and <em>mark</em>, and <em>paul</em> and <em>silas</em> can&#8217;t hear anything from <em>mark</em> and vice versa.  Let&#8217;s further suppose that <em>mark</em> had a filesystem <tt><strong>/importantstuff</strong></tt><tt> </tt>from a SAN volume mounted on it when we lost contact with it. and that <em>mark</em> is alive but out of contact.  What happens if we just go ahead and mount <tt><strong>/importantstuff</strong></tt> up on <em>paul</em>? The short answer is that bad things will happen[<a href="http://linux-ha.org/BadThingsWillHappen">2</a>]. <tt><strong>/importantstuff</strong></tt> will be irreparably corrupted as two different computers update the disk independently.  The next question you&#8217;ll ask yourself is &#8220;Where are those backup tapes?&#8221;. That&#8217;s the kind of question that&#8217;s been known to be career-ending.</p>

<p><strong>Split-Brain</strong></p>

<p>This problem of a subset of computers in a cluster beginning to operate autonomously from each other is called <em>Split Brain</em>[<a href="http://linux-ha.org/SplitBrain">3</a>]. In our example above, the cluster has split into two subclusters: {<em>paul</em>, <em>silas</em>} and {<em>mark</em>}, and each subset is unaware of the others.  This is the perhaps most difficult problem to deal with in high-availability clustering.  Although this situation does not occur frequently in practice, it does occur more often than one would guess.  As a result, it&#8217;s vital that a clustering system have a way to safely deal with this situation.</p>

<p>Earlier I mentioned that there was information you really want to know, but don&#8217;t know.  Exactly what information did I mean?   What I wanted to know was &#8220;is it safe to mount up <tt><strong>/importantstuff</strong></tt> somewhere else?&#8221;.  In turn, you could figure that out if you knew the answer to one of these two questions:  &#8220;Is <em>mark</em> really dead?&#8221; which is one way of figuring out &#8220;Is <em>mark</em> going to write on the volume any more?&#8221;  But, of course, since we can&#8217;t communicate with <em>mark</em>, this is pretty hard to figure out.  So, cluster developers came out with a kind of clever way of ensuring that this question can be answered.  We call that answer <em>fencing</em>.</p>

<p><strong>Fencing</strong></p>

<p>Fencing is the idea of putting a fence around a subcluster so that it can&#8217;t access cluster resources, like  <tt><strong>/importantstuff</strong></tt>.  If you put a fence between it and its resources, then suddenly you know the answer to the question &#8220;Is <em>mark</em> going to write on the volume any more?&#8221; - and the answer is no - because that&#8217;s what the fence is designed to prevent.  So, instead of passively wondering what the answer to the safeness question is, fencing takes action to ensure the &#8220;right&#8221; answer to the question.</p>

<p>This sort of abstract idea of fencing is fine enough, but how is this fencing stuff actually done? There are basically two general techniques:  <em>resource fencing</em> [<a href="http://linux-ha.org/ResourceFencing">4</a>] and <em>node fencing</em>.[<a href="http://linux-ha.org/NodeFencing">5</a>].
<ul>
	<li><strong>Resource fencing </strong>is the idea that if you know what resources a node might be using, then you can use some method of keeping it from accessing those resources. For example, if one has a disk which is accessed by a fiber channel switch, then one can talk to the fiber channel switch and tell it to deny the errant node access to the SAN.</li>
	<li><strong>Node fencing</strong> is the idea that one can keep a node from accessing <em>all</em> resources - without knowing what kind of resources it might be accessing, or how one might deny access to them.  A common way of doing this is to power off or reset the errant node.  This is a very effective if somewhat inelegant method of keeping it from accessing anything at all.  This technique is also called STONITH[<a href="http://linux-ha.org/STONITH">6</a>] - which is a  graphic and colorful acronym standing for Shoot The Other Node In The Head.</li>
</ul>
With fencing, we can easily keep errant nodes from accessing resources, and we can now keep the world safe for democracy - or at least keep our little corner of it safe for clustering.  An important aspect of good fencing techniques is that they&#8217;re performed without the cooperation of the node being fenced off, and that they give positive confirmation that the fencing was done.  Since errant nodes are suspect, it&#8217;s by far better to rely on positive confirmation from a correctly operating fencing component than to rely on errant cluster nodes you can&#8217;t communicate with to police themselves.</p>

<p>Although fencing is sufficient to ensure safe resource access, it is not typically considered to be sufficient for happy cluster operation because without some other mechanism, there are some behaviors it can get into which can be significantly annoying (even if your data really <em>is</em> safe).  To discuss this, let&#8217;s return our sample cluster.</p>

<p>Earlier we talked about how <em>paul</em> or <em>silas</em> could use fencing to keep the errant node <em>mark</em> from accessing <tt><strong>/importantstuff</strong></tt>.  But, what about <em>mark</em>?  If <em>mark</em> is still alive, then it is going to regard <em>paul</em> and <em>silas</em> as errant, not itself.  So, it would also proceed to fence <em>paul</em> and <em>silas</em> - and progress in the cluster would stop.  If it is using STONITH, then one could get into a sort of infinite reboot loop, with nodes declaring each other as errant and rebooting each other, coming back up and doing it all over again.  Although this is kind of humorous the first time you see this in a test environment - in production with important services, the humor of the situation probably wouldn&#8217;t be your first thought.  To solve this problem, we introduce another new mechanism - <em>quorum</em>.</p>

<p><strong>Quorum</strong></p>

<p>One way to solve the mutual fencing dilemma described above is to somehow select only one of these two subclusters to carry on and fence the subclusters it can&#8217;t communicate with.  Of course, you have to solve it without communicating with the other subclusters - since that&#8217;s the problem - you can&#8217;t communicate with them.  The idea of quorum represents the process of selecting a unique (or <em>distinguished</em> for the mathematically inclined) subcluster.</p>

<p>The most classic solution to selecting a single subcluster is a majority vote.  If you choose a subcluster with more than half of the members in it, then (barring bugs) you know there can&#8217;t be any other subclusters like this one. So, this is looks like a simple and elegant solution to the problem. For many cases, that&#8217;s true.  But, what if your cluster only has two nodes in it?  Now,  if you have a single node fail, then you can&#8217;t do anything - no one has quorum.  If this is the case, then two machines have no advantage over a single machine - it&#8217;s not much of an HA cluster.  Since 2-node HA clusters are by far the most common size of HA cluster, it&#8217;s kind of an important case to handle well.  So, how are we going to get out of this problem?</p>

<p><strong>Quorum Variants and Improvements</strong></p>

<p>What you need in this case, is some kind of a 3rd party arbitrator to help select who can fence off the other nodes and allow you to bring up resources - safely.  To solve this problem there is a variety of other methods available to act as this arbitrator - either software or hardware. Although there are several methods available to use as arbitrator, we&#8217;ll only talk about one each of hardware and software methods: <em>SCSI reserve</em> and <em>Quorum Daemon</em>.
<ul>
	<li><strong>SCSI reserve</strong>:  In hardware, we fall back on our friend SCSI reserve.  In this usage, both nodes try and reserve a disk partition available to both of them, and the SCSI reserve mechanism ensures that only one of the two of them can succeed.  Although I won&#8217;t go into all the gory details here, SCSI reserve creates its own set of problems including it won&#8217;t work reliably over geographic distances.  A disk which one uses in this way with SCSI reserve to determine quorum is sometimes called a quorum disk.  Some HA implementations (notably Microsoft&#8217;s) require a quorum disk.</li>
	<li><strong>Quorum Daemon</strong>:  In Linux-HA[<a href="http://linux-ha.org/">7]</a>, we have implemented a quorum daemon - whose sole purpose in life is to arbitrate quorum disputes between cluster members.  One could argue that for the purposes of quorum this is basically SCSI reserve implemented in software - and such an analogy is a reasonable one.  However, since it is designed for only this purpose, it has a number of significant advantages over SCSI reserve - one of which is that it can conveniently and reliably operate over geographic distances, making it ideal for disaster recovery (DR) type situations.  I&#8217;ll cover the quorum daemon and why it&#8217;s a good thing in more detail in a later posting.  Both HP and Sun have similar implementations, although I have security concerns about them, particularly over long distances.  Other than the security concerns (which might or might not concern you), both HP&#8217;s and Sun&#8217;s implementations are also good ideas.</li>
</ul>
Arguably the best way to use these alternative techniques is not directly as a quorum method, but rather as a way of breaking ties when the number of nodes in a subcluster is exactly half the number of nodes in the cluster.  Otherwise, these mechanisms can become single points of failure - that is, if they fail the cluster cannot recover.</p>

<p><strong>Alternatives to Fencing</strong></p>

<p>There are times when it is impossible to use normal 3rd-party fencing techniques.  For example, in a split-site configuration (a cluster which is split across geographically distributed sites), when inter-site communication fails, then attempts to fence will also fail.  In these cases, there are a few self-fencing alternatives which one can use when the more normal third-party fencing methods aren&#8217;t available.  These include:
<ul>
	<li><strong>Node suicide</strong>.  If a node is running resources and it loses quorum, then it can power itself off or reboot itself (sort of a self-STONITH).  The remaining nodes wait &#8220;long enough&#8221; for the other node to notice and kill itself.  The problem is that a node which is sick might not succeed in self-suicide, or might not notice that it had a membership change, or had lost quorum.   It is equally bad if notification of these events is simply delayed &#8220;too long&#8221;.  Since there is a belief that the node in question is, or at least might be, malfunctioning, this is not a trivial question.  In this case, use of hardware or software watchdog timers becomes critical.</li>
	<li><strong>Self-shutdown</strong>.  This self-fencing method is a variant on suicide, except that resources are stopped gracefully.  It has many of the same problems, except it is somewhat less reliable because the time to shut down resources can be quite long.  Like the case above, use of hardware or software watchdog timers becomes critical.</li>
</ul>
Note that without fencing, the membership and quorum algorithms are extremely critical.  You&#8217;ve basically lost a layer of protection, and you&#8217;ve switched from relying on a component which gives positive confirmation to relying on a probably faulty component to fence itself, and then hoping without confirmation that you&#8217;ve waited long enough before continuing.</p>

<p><strong>Summary</strong></p>

<p>Split-brain is the idea that a cluster can have communication failures, which can cause it to split into subclusters.  Fencing is the way of ensuring that one can safely proceed in these cases, and quorum is the idea of determining which subcluster can fence the others and proceed to recover the cluster services.</p>

<p><strong>An Important Final Note</strong></p>

<p>It is fencing which best guarantees the safety of your resources.  Nothing else works quite as well.  If you have fencing in your cluster software, and you have irreparable resources (i.e. that would be irreparably damaged in a split-brain situation), then you <strong>must</strong> configure fencing.  If your HA software doesn&#8217;t support (3rd party) fencing, then I suggest that you consider getting a different HA package.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SBD Fencing]]></title>
    <link href="http://github.klwang.info/blog/sbd-fencing/"/>
    <updated>2013-05-19T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/sbd-fencing</id>
    <content type="html"><![CDATA[<p>原文链接 <a href="http://www.linux-ha.org/wiki/SBD_Fencing">SBD Fencing</a></p>

<p>SBD expands to storage-based death, and is named in reference to Novell&#8217;s Cluster Services, which used SBD to exchange poison pill messages.</p>

<p>The sbd daemon, combined with the external/sbd STONITH agent, provides a way to enable STONITH and fencing in clusters without external power switches, but with shared storage.</p>

<p>The sbd daemon runs on all nodes in the cluster, monitoring the shared storage. When it either loses access to the majority of sbd devices, or sees that another node has written a fencing request to its mailbox slot, the node will immediately fence itself.</p>

<p>sbd can be used in virtual environments where the hypervisor layer is not cluster-enabled, but a shared storage device between the guests is available; for other scenarios, please see DomUClusters.</p>

<p>While this mechanism requires minimal cooperation from the node to be fenced, the code has proven very stable over the course of several years.
<h4>Requirements</h4>
You must have shared storage.<br />
You can use one, two, or three devices.<br />
This can be connected via Fibre Channel, Fibre Channel over Eterhnet, or even iSCSI.</p>

<p>Thus, an iSCSI target can become a sort-of network-based quorum server; the advantage is that it does not require a smart host at your third location, just block storage.</p>

<p>You must dedicate a small partition of each as the SBD device.<br />
The SBD devices must not make use of host-based RAID.<br />
The SBD devices must not reside on a DRBD instance.</p>

<p>Why? Because DRBD is not shared, but replicated storage. If your cluster communication breaks down, and you finally need to actually use stonith, chances are that the DRBD replication link broke down as well, whatever you write to your local instance of DRBD cannot reach the peer, and the peer&#8217;s sbd daemon has no way to know about that poison pill it is supposed to commit suicide uppon.<br />
The SBD device may of course be an iSCSI LU, which in turn may be exported from a DRBD based iSCSI target cluster.</p>

<p>A SBD device can be shared between different clusters, as long as no more than 255 nodes share the device.
<h4>Configuration</h4>
How many devices should I use?</p>

<p>SBD supports one, two, or three devices. This affects the operation of SBD as follows:<br />
One device</p>

<p>In its most simple implementation, you use one device only. (Older versions of SBD did not support more.) This is appropriate for clusters where all your data is on the same shared storage (with internal redundancy) anyway; the SBD device does not introduce an additional single point of failure then.<br />
Three devices</p>

<p>In this most reliable configuration, SBD will only commit suicide if more than one device is lost; hence, this configuration is resilient against one device outages (be it due to failures or maintenance). Fencing messages can be successfully relayed if at least two devices remain up.</p>

<p>This configuration is appropriate for more complex scenarios where storage is not confined to a single array.</p>

<p>Host-based mirroring solutions could have one SBD per mirror leg (not mirrored itself), and an additional tie-breaker on iSCSI.</p>

<p>Two devices</p>

<p>This configuration is a trade-off, primarily aimed at environments where host-based mirroring is used, but no third storage device is available.</p>

<p>SBD will not commit suicide if it loses access to one mirror leg; this allows the cluster to continue to function even in the face of one outage.</p>

<p>However, SBD will not fence the other side while only one mirror leg is available, since it does not have enough knowledge to detect an asymmetric split of the storage. So it will not be able to automatically tolerate a second failure while one of the storage arrays is down. (Though you can use the appropriate crm command to acknowledge the fence manually.)<br />
Initialize the sbd device(s)</p>

<p>All these steps must be performed as root.</p>

<p>Decide which block device(s) will serve as the SBD device(s). This can be a logical unit, partition, or a logical volume; but it must be accessible from all nodes. Substitute the full path to this device wherever /dev/sbd is referenced below.</p>

<p>If you have selected more than one device, provide them by specifying the -d options multiple times, as in: sbd -d /dev/sda -d /dev/sdb -d /dev/sdc &#8230;</p>

<p>After having made very sure that these are indeed the devices you want to use, and do not hold any data you need - as the sbd command will overwrite it without further requests for confirmation -, initialize the sbd devices:
<pre> # sbd -d /dev/sbd create
 # sbd -d /dev/sbd3 -d /dev/sdc2 -d /dev/disk/by-id/foo-part1 create</pre>
This will write a header to the device, and create slots for up to 255 nodes sharing this device. You can look at what was written to the device using:
<pre> # sbd -d /dev/sbd dump
 Header version     : 2
 Number of slots    : 255
 Sector size        : 512
 Timeout (watchdog) : 5
 Timeout (allocate) : 2
 Timeout (loop)     : 1
 Timeout (msgwait)  : 10</pre>
As you can see, the timeouts are also stored in the header, to ensure that all participating nodes agree on them.
<h4>Setup the software watchdog</h4>
It is most strongly suggested that you set up your Linux system to use a watchdog. Use the watchdog driver, which fits best to your hardware, e. g. hpwdt for HP server. A list of available watchdogs can be found in /usr/src/linux/drivers/watchdog/</p>

<p>If no watchdog matches to your hardware then use softdog.</p>

<p>You can do this by adding the line
<pre> modprobe softdog</pre>
to
<pre> /etc/init.d/boot.local</pre>
<h4>Start the sbd daemon</h4>
The sbd daemon is a critical piece of the cluster stack. It must always be running when the cluster stack is up, or even when it has crashed.</p>

<p>The heartbeat/openais init script starts and stops SBD if configured; add the following to /etc/sysconfig/sbd:
<pre> SBD_DEVICE="/dev/sbd"
 SBD_OPTS="-W"</pre>
-W enables the watchdog support, which you are most strongly suggested to do. If you need to specify multiple devices here, use a semicolon to separate them (their order does not matter):
<pre> SBD_DEVICE="/dev/sbd;/dev/sde;/dev/sdc"</pre>
If the SBD device is not accessible, the daemon will fail to start and prevent the cluster stack from coming up, too.
<h4>Testing the sbd daemon</h4>
<pre> sbd -d /dev/sbd list</pre>
Will dump the node slots, and their current messages, from the sbd device. You should see all cluster nodes being listed there; most likely with a message clear.</p>

<p>You can now try sending a test message to one of the nodes:
<pre> sbd -d /dev/sbd message nodea test</pre>
The node will acknowledge the receipt of the message in the system logs:
<pre> Aug 29 14:10:00 nodea sbd: [13412]: info: Received command test from nodeb</pre>
Messages are considered to have been delivered successfully if they have been sent to more than half of the configured devices.
<h4>Configure the fencing resource</h4>
All that is required is to add a STONITH resource of type external/sbd to the CIB. Newer versions of the agent will automatically source the devices from the host&#8217;s /etc/sysconfig/sbd. If this does not match your configuration, or if you are running an older version of the agent, set the sbd_device instance attribute accordingly.</p>

<p>Sample configuration for crm configure:
<pre> primitive stonith_sbd stonith:external/sbd</pre>
Or, if you need to specify the device name:
<pre> primitive stonith_sbd stonith:external/sbd \
 	params sbd_device="/dev/sbd"</pre>
The sbd agent does not need to and should not be cloned. If all of your nodes run SBD, as is most likely, not even a monitor action provides a real benefit, since the daemon would suicide the node if there was a problem.</p>

<p>SBD also supports turning the reset request into a crash request, which may be helpful for debugging if you have kernel crashdumping configured; then, every fence request will cause the node to dump core. You can enable this via the crashdump=&#8221;true&#8221; setting on the fencing resource. This is not recommended for on-going production use, but for debugging phases.
<h4>Multipathing</h4>
If your single sbd device resides on a multipath group, you may need to adjust the timeouts sbd uses, as MPIO&#8217;s path down detection can cause delays. (If you have multiple devices, transient timeouts of a single device will not negatively affect SBD. However, if they all go through the same FC switches, you will still need to do this.)</p>

<p>After the msgwait timeout, the message is assumed to have been delivered to the node. For multipath, this should be the time required for MPIO to detect a path failure and switch to the next path. You may have to test this in your environment.</p>

<p>The node will perform suicide if it has not updated the watchdog timer fast enough; the watchdog timeout must be shorter than the msgwait timeout - half the value is a good rule of thumb.</p>

<p>You would set these values by adding -4 msgwait -1 watchdogtimeout to the create command:
<pre> /usr/sbin/sbd -d /dev/sbd -4 20 -1 10 create</pre>
(All timeouts are in seconds.)</p>

<p>Note: This can incur significant delays to fail-over, unfortunately.
<h4>Recovering from temporary device outages</h4>
If you have multiple devices, failure of a single device is not immediately fatal. SBD will retry ten times in succession to reattach to the device, and then pause (as to not flood the system) for an hour before retrying. Thus, SBD should automatically recover from temporary outages.</p>

<p>Should you wish to try reattach to the device right now, you can send a SIGUSR1 to the SBD parent daemon.</p>

<p>The timeout can be tuned via the -t 3600 option. Setting the timeout to 0 will disable automatic restarts.
<h4>Limitations</h4>
Again, the sbd device must not use host-based RAID.<br />
sbd is currently limited to 255 nodes per partition. If you have a need to configure larger clusters, create multiple sbd partitions, split the watch daemons across them, and configure one external/sbd STONITH resource per sbd device.
<h4>Misc</h4>
Slot allocation is automatic; when a daemon is started in watch mode, it will allocate one slot for itself if needed and then monitor it for incoming requests.</p>

<p>Similarly, no hostlist has to be provided to external/sbd; it retrieves this list automatically from the sbd device.</p>

<p>To overcome the MPIO delays, sbd should handle several paths internally, submitting the requests to all paths concurrently. However, this is not quite as trivial, as IO ordering is not guaranteed.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[将wordpress博客站点移动到子目录]]></title>
    <link href="http://github.klwang.info/blog/move-my-wordpress-to-a-subdir/"/>
    <updated>2013-05-19T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/move-my-wordpress-to-a-subdir</id>
    <content type="html"><![CDATA[<p>之前为了简单，建立博客时使用了wordpress的默认选项，一路回车；</p>

<p>今天心血来潮，想要安装新的功能，才发现网站的目录结构已经惨不忍睹了，索性就将wp移动到了子目录</p>

<p>1. 修改 WordPress 地址（URL） (设置-> 常规)
<pre>       http://klwang.info/abcd     --这里假设站点的子目录是abcd</pre>
2. 将所有的wp文件全部移动到bacd目录</p>

<p>3. 将原来的index.php和.<tt>.htaccess</tt>文件复制到根目录<br />
    注意：这里是<strong>复制</strong>
    想知道直接移动是啥效果吗，可以试试，会发现自己的博客赤裸裸的暴露在了攻击之下</p>

<p>4. 用文本编辑器打开根目录中的 index.php, 原文
<pre>       require('./wp-blog-header.php');</pre>
修改成
<pre>       require('./abcd/wp-blog-header.php');</pre></p>

<p>5.更新固定链接 (设置->固定链接)<br />
直接保存即可 (这一步很重要，不然，你会发现之前的文章都不能访问了)</p>

<p>参考文章:  <a href="http://codex.wordpress.org/Giving_WordPress_Its_Own_Directory">Giving WordPress Its Own Directory</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu server 安装 DHCP服务器]]></title>
    <link href="http://github.klwang.info/blog/install-a-dhcp-service-on-ubuntu-server/"/>
    <updated>2013-05-19T00:00:00+08:00</updated>
    <id>http://github.klwang.info/blog/install-a-dhcp-service-on-ubuntu-server</id>
    <content type="html"><![CDATA[<p>1. 基本软件包安装
<pre>sudo apt-get install isc-dhcp-server</pre>
2. dhcp配置
<pre>default-lease-time 600;
max-lease-time 7200;
subnet 192.168.1.0 netmask 255.255.255.0 {
range 192.168.1.150 192.168.1.200;                     --地址范围
option routers 192.168.1.254;                          --gateway地址，可选
option domain-name-servers 192.168.1.1, 192.168.1.2;   --dns服务器地址，可选
option domain-name "mydomain.example";                 --域名，可选
}</pre>
3. 重启服务
<pre>sudo service isc-dhcp-server restart</pre></p>
]]></content>
  </entry>
  
</feed>
